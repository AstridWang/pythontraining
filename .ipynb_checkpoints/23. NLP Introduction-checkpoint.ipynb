{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言处理介绍及实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/nlp.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n",
    "\n",
    "自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为data analyst，我们日常中的工作，很大一部分就是将信息从交易所、上市公司、基金公司公布的金融文档中提取出来。\n",
    "\n",
    "比如基金名称，具体的林林总总的金融数据等，如果掌握自然语言处理技巧，或许能够对日常工作如虎添翼。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 主要范畴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本朗读（Text to speech）/语音合成（Speech synthesis）\n",
    "\n",
    "语音识别（Speech recognition）\n",
    "\n",
    "中文自动分词（Chinese word segmentation）\n",
    "\n",
    "词性标注（Part-of-speech tagging）\n",
    "\n",
    "句法分析（Parsing）\n",
    "\n",
    "自然语言生成（Natural language generation）\n",
    "\n",
    "文本分类（Text categorization）\n",
    "\n",
    "信息检索（Information retrieval）\n",
    "\n",
    "信息抽取（Information extraction）\n",
    "\n",
    "文字校对（Text-proofing）\n",
    "\n",
    "问答系统（Question answering）\n",
    "\n",
    "机器翻译（Machine translation）\n",
    "\n",
    "自动摘要（Automatic summarization）\n",
    "\n",
    "文字蕴涵（Textual entailment）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/nlparc.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 常用套路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 收集数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于我们analyst来说，就是从我们文档库里面，把我们关心的filing收集起来，然后最好按照句子为单位作为样本进行堆叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们拿如下这一段话，进行分句：\n",
    "\n",
    "This prospectus offers variable annuity contract allowing you to accumulate values and paying you benefits on a variable and/or fixed basis. This prospectus provides information regarding the material provisions of your variable annuity contract. We may restrict the availability of this contract to certain broker-dealers. National Security Life V.I. and Annuity Company (\"National Security\") issues the contract. This contract is only available in New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitparagraph2sentence(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    return [sentence.text for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This prospectus offers variable annuity contract allowing you to accumulate values and paying you benefits on a variable and/or fixed basis.\n",
      "This prospectus provides information regarding the material provisions of your variable annuity contract.\n",
      "We may restrict the availability of this contract to certain broker-dealers.\n",
      "National Security Life V.I. and Annuity Company (\"National Security\") issues the contract.\n",
      "This contract is only available in New York.\n"
     ]
    }
   ],
   "source": [
    "sentences = splitparagraph2sentence('This prospectus offers variable annuity contract allowing you to accumulate values and paying you benefits on a variable and/or fixed basis. This prospectus provides information regarding the material provisions of your variable annuity contract. We may restrict the availability of this contract to certain broker-dealers. National Security Life V.I. and Annuity Company (\"National Security\") issues the contract. This contract is only available in New York.')\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：National Security Life V.I.中的点，没有被无脑作为分句的依据，而是真正根据语义分句。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>如果能够做有监督的分类，就顺手打上标签</b>，因为做无监督的聚类操作，然后根据相似度判断文本类型，耗时耗力，而且效果不是很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 清洗数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们遵循的第一原则是：“再好的模型也拯救不了shi一样的数据”。所以，先来清洗一下数据吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们做以下处理：\n",
    "准则：去除变量，只留常量，或者可常量化。\n",
    "\n",
    "1. 删除所有不相关的字符，如任何非字母数字字符\n",
    "\n",
    "2. 通过文本分隔分成单独的单词来标记你的文章\n",
    "\n",
    "3. 删除不相关的字词，例如“@”推特或网址\n",
    "\n",
    "4. 将所有字符转换为小写字母，以便将诸如“hello”，“Hello”和“HELLO”等单词看做相同单词\n",
    "\n",
    "5. 考虑整合拼写错误或多种拼写的单词，用一个单词代表（例如“cool”/“kewl”/“cooool”）相结合\n",
    "\n",
    "6. 考虑词形还原（把“am”，“are”，“is”等词语缩小为“be”这样的常见形式）\n",
    "\n",
    "7. 将所有专有名词转换为propn这个语义标注词，即变量转换为常量！\n",
    "\n",
    "8. 去除停用词，比如for a an of the and to about after in among as..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体实现方式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除所有非字母的字符\n",
    "如这句话：Also assume that, when the Owner is age 76, a step up occurs and the highest quarterly Contract Value is greater than the BDB; in that case, the GAWA percentage will be re determined based on the Owner's attained age of 76, resulting in a new GAWA percentage of 6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Also assume that when the Owner is age a step up occurs and the highest quarterly Contract Value is greater than the BDB in that case the GAWA percentage will be re determined based on the Owner s attained age of resulting in a new GAWA percentage of\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = '''Also assume that, when the Owner is age 76, a step up occurs and the highest quarterly Contract Value is greater than the BDB; in that case, the GAWA percentage will be re determined based on the Owner's attained age of 76, resulting in a new GAWA percentage of 6%.'''\n",
    "text = re.sub(r'\\W', ' ', text)\n",
    "text = re.sub(r'\\d', ' ', text)\n",
    "text = re.sub(r'( ){2,}', ' ', text).strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性还原"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是词性？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词性指以词的特点作为划分词类的根据，比如：\n",
    "ADV: 副词；sample：very, well, exactly, tomorrow, up, down\n",
    "\n",
    "VERB: 动词；sample: run, eat, ate, running, eats\n",
    "\n",
    "ADJ: 形容词；sample: big, old, green\n",
    "\n",
    "DET: 限定词；sample: a, an, this, this, no\n",
    "\n",
    "NOUN: 名词；sample: girl, boy, cat, tree\n",
    "\n",
    "ADP: 介词；sample: in, to, during\n",
    "\n",
    "PROPN: 专属名词；sample: Mary, London, HBO, Google\n",
    "\n",
    "CCONJ: 连词；sample: and, or, but\n",
    "\n",
    "参照：http://universaldependencies.org/u/pos/all.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的例子，演示如何通过Spacy获取一句话中各个单词的词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwordtokenattributes(text):\n",
    "    doc = nlp(text)\n",
    "    result = []\n",
    "    wordlist = []\n",
    "    for token in doc:\n",
    "#         if token.text not in wordlist:\n",
    "        dictinfo = {}\n",
    "        dictinfo['text'] = token.text\n",
    "        dictinfo['lemma_'] = token.lemma_\n",
    "        dictinfo['pos_'] = token.pos_\n",
    "        dictinfo['tag_'] = token.tag_\n",
    "        dictinfo['dep_'] = token.dep_\n",
    "        dictinfo['shape_'] = token.shape_\n",
    "        dictinfo['is_alpha'] = token.is_alpha\n",
    "        dictinfo['is_stop'] = token.is_stop\n",
    "        wordlist.append(token.text)\n",
    "        result.append(dictinfo)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getwordtokenattributes(r'I cook with new best cook. A good cook cooks a good cook.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'I', 'lemma_': '-PRON-', 'pos_': 'PRON', 'tag_': 'PRP', 'dep_': 'nsubj', 'shape_': 'X', 'is_alpha': True, 'is_stop': False}, {'text': 'cook', 'lemma_': 'cook', 'pos_': 'VERB', 'tag_': 'VBP', 'dep_': 'ROOT', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': 'with', 'lemma_': 'with', 'pos_': 'ADP', 'tag_': 'IN', 'dep_': 'prep', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': 'new', 'lemma_': 'new', 'pos_': 'ADJ', 'tag_': 'JJ', 'dep_': 'amod', 'shape_': 'xxx', 'is_alpha': True, 'is_stop': False}, {'text': 'best', 'lemma_': 'good', 'pos_': 'ADJ', 'tag_': 'JJS', 'dep_': 'amod', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': 'cook', 'lemma_': 'cook', 'pos_': 'NOUN', 'tag_': 'NN', 'dep_': 'pobj', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': '.', 'lemma_': '.', 'pos_': 'PUNCT', 'tag_': '.', 'dep_': 'punct', 'shape_': '.', 'is_alpha': False, 'is_stop': False}, {'text': 'A', 'lemma_': 'a', 'pos_': 'DET', 'tag_': 'DT', 'dep_': 'det', 'shape_': 'X', 'is_alpha': True, 'is_stop': False}, {'text': 'good', 'lemma_': 'good', 'pos_': 'ADJ', 'tag_': 'JJ', 'dep_': 'amod', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': 'cook', 'lemma_': 'cook', 'pos_': 'NOUN', 'tag_': 'NN', 'dep_': 'nsubj', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': 'cooks', 'lemma_': 'cook', 'pos_': 'NOUN', 'tag_': 'NNS', 'dep_': 'ROOT', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': 'a', 'lemma_': 'a', 'pos_': 'DET', 'tag_': 'DT', 'dep_': 'det', 'shape_': 'x', 'is_alpha': True, 'is_stop': False}, {'text': 'good', 'lemma_': 'good', 'pos_': 'ADJ', 'tag_': 'JJ', 'dep_': 'amod', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': 'cook', 'lemma_': 'cook', 'pos_': 'NOUN', 'tag_': 'NN', 'dep_': 'dobj', 'shape_': 'xxxx', 'is_alpha': True, 'is_stop': False}, {'text': '.', 'lemma_': '.', 'pos_': 'PUNCT', 'tag_': '.', 'dep_': 'punct', 'shape_': '.', 'is_alpha': False, 'is_stop': False}]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以用Pandas的DataFrame，将结果变得容易阅读："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep_</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>pos_</th>\n",
       "      <th>shape_</th>\n",
       "      <th>tag_</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>X</td>\n",
       "      <td>PRP</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROOT</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>cook</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>VBP</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>IN</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amod</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>new</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>xxx</td>\n",
       "      <td>JJ</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amod</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>good</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>JJS</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pobj</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>cook</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>NN</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>punct</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>det</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>X</td>\n",
       "      <td>DT</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>amod</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>good</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>JJ</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>cook</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>NN</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROOT</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>cook</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>NNS</td>\n",
       "      <td>cooks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>det</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>x</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amod</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>good</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>JJ</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dobj</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>cook</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>NN</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>punct</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dep_  is_alpha  is_stop  lemma_   pos_ shape_ tag_   text\n",
       "0   nsubj      True    False  -PRON-   PRON      X  PRP      I\n",
       "1    ROOT      True    False    cook   VERB   xxxx  VBP   cook\n",
       "2    prep      True    False    with    ADP   xxxx   IN   with\n",
       "3    amod      True    False     new    ADJ    xxx   JJ    new\n",
       "4    amod      True    False    good    ADJ   xxxx  JJS   best\n",
       "5    pobj      True    False    cook   NOUN   xxxx   NN   cook\n",
       "6   punct     False    False       .  PUNCT      .    .      .\n",
       "7     det      True    False       a    DET      X   DT      A\n",
       "8    amod      True    False    good    ADJ   xxxx   JJ   good\n",
       "9   nsubj      True    False    cook   NOUN   xxxx   NN   cook\n",
       "10   ROOT      True    False    cook   NOUN   xxxx  NNS  cooks\n",
       "11    det      True    False       a    DET      x   DT      a\n",
       "12   amod      True    False    good    ADJ   xxxx   JJ   good\n",
       "13   dobj      True    False    cook   NOUN   xxxx   NN   cook\n",
       "14  punct     False    False       .  PUNCT      .    .      ."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(result)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过词性还原获得语干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(sentence, allowed_postags=''):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    # allowed_postags, such as 'NOUN,ADJ,VERB,ADV',\n",
    "    # 但是大多数情况，不能加allow_postags，否则很多词，比如no,  or就没有了\n",
    "    if len(allowed_postags) > 0:\n",
    "        resultlist = [token.lemma_\n",
    "                      for token\n",
    "                      in doc\n",
    "                      if token.pos_\n",
    "                      in [postag.upper().strip() for postag in allowed_postags.split(',')]]\n",
    "    else:\n",
    "        resultlist =  [token.lemma_ for token in doc]\n",
    "    return resultlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r'The product is the best than others.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the product be the good than other .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(lemmatization(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过词性表达式获得短语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "def extractverbphrase(text, pattern=r'(<ADV>*<NOUN|PROPN>*<VERB><DET>?<ADV>*<VERB|ADJ>+<ADP>?<DET>?<NUM>*<ADJ>*<NOUN|PROPN>*<ADV>?)|(<VERB>?<NOUN|PROPN>*<ADV>?<VERB><ADP>?<ADJ|VERB>*<ADP>?<DET>?<VERB>?<NOUN|PROPN>*)|(<DET>?<ADJ>+<NOUN|PROPN>+)|(<ADV>*<ADJ><ADP><DET>?<VERB|ADJ>*<NOUN|PROPN>*)|(<DET><NOUN><CCONJ><NOUN>)|(<NOUN|PROPN>*<CCONJ>?<NOUN|PROPN>+<ADP><NOUN|PROPN>+)|(<ADP><DET><NOUN|PROPN>+)'):\n",
    "    # ADV: 副词；sample：very, well, exactly, tomorrow, up, down\n",
    "    # VERB: 动词；sample: run, eat, ate, running, eats\n",
    "    # ADJ: 形容词；sample: big, old, green\n",
    "    # DET: 限定词；sample: a, an, this, this, no\n",
    "    # NOUN: 名词；sample: girl, boy, cat, tree\n",
    "    # ADP: 介词；sample: in, to, during\n",
    "    # PROPN: 专属名词；sample: Mary, London, HBO, Google\n",
    "    # CCONJ: 连词；sample: and, or, but\n",
    "    # 参照：http://universaldependencies.org/u/pos/all.html\n",
    "    doc = nlp(text)\n",
    "    return list(textacy.extract.pos_regex_matches(doc, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April\n",
      "Investment Divisions\n",
      "Accumulation Unit\n"
     ]
    }
   ],
   "source": [
    "text = r'Effective April 24, 2017, there are new Investment Divisions for which Accumulation Unit information is not yet available.'\n",
    "phraselist = extractverbphrase(text, pattern=r'(<PROPN>+)')\n",
    "for phrase in phraselist:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统一的文字清洗方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将清理逻辑连接起来，构成一个统一的文字清洗方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removespecialchar(sentence):\n",
    "    result = re.sub('\\W', ' ', sentence)\n",
    "    return re.sub('( ){2,}', ' ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearandlemmasentence(sentence,\n",
    "                          stopword='for a an the and in among'):\n",
    "    stoplist = set(stopword.split())\n",
    "    sentence = removespecialchar(sentence).lower().strip()\n",
    "    sentence = ' '.join([word.strip() for word\n",
    "                         in sentence.lower().strip().split()\n",
    "                         if len(word.strip()) > 0\n",
    "                         and word not in stoplist]).strip()\n",
    "    sentence = re.sub(r'(propn\\s+){2,}', 'propn ', sentence)\n",
    "    if len(sentence) == 0:\n",
    "        sentence = 'only for test'\n",
    "    lemmawordlist = lemmatization(sentence)\n",
    "    return lemmawordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacevariabletextfromtextblock(textblock):\n",
    "    \"\"\"\n",
    "    Variable Text:\n",
    "    1. PROPN words, such as: Mainstay VP Funds Trust, replace them with propn\n",
    "    2. Date part, such as January 1, 2018, replace them with date\n",
    "    3. Number, such as 1, 2, replace with space\n",
    "    :param textblock:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # replace date string with \"date\"\n",
    "    datepattern = r'((January|February|March|April|May|June|July|August|September|October|November|December)[\\s]*[0-9]{1,2}[\\s]*,[\\s]*[0-9]{4})|([0-9]{1,2}/[0-9]{1,2}/[0-9]{4})'\n",
    "    textblock = re.sub(datepattern, 'date', textblock)\n",
    "    datepattern = r'\\d{2}\\/\\d{2}\\/(\\d{4}|\\d{2})'\n",
    "    textblock = re.sub(datepattern, 'date', textblock)\n",
    "    # 应对*CTIVP这种情况，无法识别PROPN\n",
    "    textblock = textblock.replace('*', ' ')\n",
    "    textblock = re.sub(r'( ){2,}', ' ', textblock).strip()\n",
    "    # 因为Money Market Fund前缀与后缀词经常是具体的基金公司，\n",
    "    # 所以去除具体基金公司名称的同时，\n",
    "    # 避免其被作为专属名词替换\n",
    "    textblock = textblock.replace(' of ', ' ')\\\n",
    "        .replace(' Inc. ', ' ')\\\n",
    "        .replace('&', '')\\\n",
    "        .replace(' LLC ', ' ')\n",
    "    textblock = textblock.replace('-', ' ').replace('–', ' ')\n",
    "    textblock = re.sub(r'( ){2,}', ' ', textblock).strip()\n",
    "    phraselist = extractverbphrase(textblock, '<PROPN>+')\n",
    "    phraselist.sort(key=lambda i: len(i), reverse=True)\n",
    "    if len(phraselist) > 0:\n",
    "        for phrase in phraselist:\n",
    "            phrasetext = phrase.text\n",
    "            # avoid remove important words which are related with category\n",
    "            if 'money market fund' in phrasetext.lower():\n",
    "                textblock = textblock.replace(phrasetext, 'money market fund')\n",
    "            noexcludewordlist = ['date',\n",
    "                                 ' merge ',\n",
    "                                 ' merged ',\n",
    "                                 ' merging ',\n",
    "                                 ' merger ',\n",
    "                                 'acquir',\n",
    "                                 'surviv',\n",
    "                                 'liquidat',\n",
    "                                 'transfer',\n",
    "                                 'reorganiz',\n",
    "                                 'expense table',\n",
    "                                 'fee summary',\n",
    "                                 'operating expenses',\n",
    "                                 'annual fund',\n",
    "                                 'the adviser',\n",
    "                                 'benefit payment',\n",
    "                                 'variable account option']\n",
    "            shouldignore = False\n",
    "            for word in noexcludewordlist:\n",
    "                if word in phrasetext.lower():\n",
    "                    shouldignore = True\n",
    "                    break\n",
    "            if shouldignore:\n",
    "                continue\n",
    "            if not any([phrasetext.lower() == 'fund',\n",
    "                        len(phrasetext.split()) <= 2]):\n",
    "                textblock = textblock.replace(phrasetext, 'propn')\n",
    "    textblock = textblock.replace('PIMCO', ' ')\n",
    "    textblock = re.sub(r'\\W', ' ', textblock)\n",
    "    textblock = re.sub(r'\\d', ' ', textblock)\n",
    "    textblock = re.sub(r'(propn\\s+){2,}', 'propn ', textblock)\n",
    "    textblock = re.sub(r'( ){2,}', ' ', textblock).strip()\n",
    "    return textblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleardatafordoc2vector(doc):\n",
    "    temp = ' '.join(\n",
    "        clearandlemmasentence(replacevariabletextfromtextblock(doc),\n",
    "                              'for a an of the and or to about after in among as at be been was were is are being b c d e f g h i j k l m n o p q r s t u v w x y z'\n",
    "                              )).strip()\n",
    "    temp = temp.replace('-PRON-', 'pron')\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以做一下效果测试：<br>\n",
    "原句：122 66 32 15 14 5 13 17 *CTIVP SM – Eaton Vance Floating Rate Income Fund (Class 2) liquidated on April 27, 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propn class liquidate on date\n"
     ]
    }
   ],
   "source": [
    "text = r'122 66 32 15 14 5 13 17 *CTIVP SM – Eaton Vance Floating Rate Income Fund (Class 2) liquidated on April 27, 2018. '\n",
    "print(cleardatafordoc2vector(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 找到一个好的数据表示方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 词袋化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words模型是信息检索领域常用的文档表示方法。\n",
    "\n",
    "在信息检索中，BOW模型假定对于一个文档，忽略它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的，不依赖于其它单词是否出现。\n",
    "\n",
    "也就是说，文档中任意一个位置出现的任何单词，都不受该文档语意影响而独立选择的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋模型的缺点："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋模型最重要的是构造词表，然后通过文本为词表中的词赋值，但词袋模型严重缺乏相似词之间的表达。 \n",
    "\n",
    "比如“我喜欢北京”“我不喜欢北京”其实这两个文本是严重不相似的。但词袋模型会判为高度相似。 \n",
    "\n",
    "“我喜欢北京”与“我爱北京”其实表达的意思是非常非常的接近的，但词袋模型不能表示“喜欢”和“爱”之间严重的相似关系。（当然词袋模型也能给这两句话很高的相似度，但是注意我想表达的含义）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Investment名字相似度这个应用中，正是采用了词袋 + TF/IDF模型 + 余弦相似度作为核心。因为单纯的investment并不存在或者很少存在需要语义分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python36\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "namelist = [\n",
    "    'ODDO BHF US Mid Cap CI-EUR H',\n",
    "    'ODDO BHF US Mid Cap CR-USD',\n",
    "    'Credit Suisse Index Fund (CH) - CSIF (CH) Bond Fiscal Strength EUR Blue ZA',\n",
    "    'Winton Diversified Futures Fund (Luxembourg) C GBP Acc',\n",
    "    'Prescient Core Equity Fund B5',\n",
    "    'Robeco QI GTAA Plus DHL $',\n",
    "    'Franklin US Rising Dividends T',\n",
    "    'FT MLP Closed-End Fund & Energy 52 CA',\n",
    "    'FT Richard Bern Adv TS Amer Ind 16-3 CA',\n",
    "    'FT Municipal FT Income Select CE 81 CA',\n",
    "    'Raiffeisen-Pensionsfonds-Österreich 2007 VT',\n",
    "    'Multipartner SICAV - Carthesio Asian Credit Fund B EUR',\n",
    "    'HSBC Wealth Strategic Solutions Fund (1) - Conservative Portfolio Income X',\n",
    "    'American Beacon Flexible Bond Fund A Class',\n",
    "    'Robeco QI GTAA Plus IHL $',\n",
    "    'AXA World Funds - Global Equity Income M Capitalisation EUR']\n",
    "stoplist = set('for a an of the and to in - $ &'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ODDO', 'BHF', 'US', 'Mid', 'Cap', 'CI-EUR', 'H'], ['ODDO', 'BHF', 'US', 'Mid', 'Cap', 'CR-USD'], ['Credit', 'Suisse', 'Index', 'Fund', '(CH)', 'CSIF', '(CH)', 'Bond', 'Fiscal', 'Strength', 'EUR', 'Blue', 'ZA'], ['Winton', 'Diversified', 'Futures', 'Fund', '(Luxembourg)', 'C', 'GBP', 'Acc'], ['Prescient', 'Core', 'Equity', 'Fund', 'B5'], ['Robeco', 'QI', 'GTAA', 'Plus', 'DHL'], ['Franklin', 'US', 'Rising', 'Dividends', 'T'], ['FT', 'MLP', 'Closed-End', 'Fund', 'Energy', '52', 'CA'], ['FT', 'Richard', 'Bern', 'Adv', 'TS', 'Amer', 'Ind', '16-3', 'CA'], ['FT', 'Municipal', 'FT', 'Income', 'Select', 'CE', '81', 'CA'], ['Raiffeisen-Pensionsfonds-Österreich', '2007', 'VT'], ['Multipartner', 'SICAV', 'Carthesio', 'Asian', 'Credit', 'Fund', 'B', 'EUR'], ['HSBC', 'Wealth', 'Strategic', 'Solutions', 'Fund', '(1)', 'Conservative', 'Portfolio', 'Income', 'X'], ['American', 'Beacon', 'Flexible', 'Bond', 'Fund', 'A', 'Class'], ['Robeco', 'QI', 'GTAA', 'Plus', 'IHL'], ['AXA', 'World', 'Funds', 'Global', 'Equity', 'Income', 'M', 'Capitalisation', 'EUR']]\n"
     ]
    }
   ],
   "source": [
    "data_train = []\n",
    "for name in namelist:\n",
    "    data_train.append([word for word in name.strip().split() \n",
    "                       if word not in stoplist])\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码演示如何生成词袋字典以及词袋模型，并保存为具体的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出每个单词对应的索引编号\n",
      "{'BHF': 0, 'CI-EUR': 1, 'Cap': 2, 'H': 3, 'Mid': 4, 'ODDO': 5, 'US': 6, 'CR-USD': 7, '(CH)': 8, 'Blue': 9, 'Bond': 10, 'CSIF': 11, 'Credit': 12, 'EUR': 13, 'Fiscal': 14, 'Fund': 15, 'Index': 16, 'Strength': 17, 'Suisse': 18, 'ZA': 19, '(Luxembourg)': 20, 'Acc': 21, 'C': 22, 'Diversified': 23, 'Futures': 24, 'GBP': 25, 'Winton': 26, 'B5': 27, 'Core': 28, 'Equity': 29, 'Prescient': 30, 'DHL': 31, 'GTAA': 32, 'Plus': 33, 'QI': 34, 'Robeco': 35, 'Dividends': 36, 'Franklin': 37, 'Rising': 38, 'T': 39, '52': 40, 'CA': 41, 'Closed-End': 42, 'Energy': 43, 'FT': 44, 'MLP': 45, '16-3': 46, 'Adv': 47, 'Amer': 48, 'Bern': 49, 'Ind': 50, 'Richard': 51, 'TS': 52, '81': 53, 'CE': 54, 'Income': 55, 'Municipal': 56, 'Select': 57, '2007': 58, 'Raiffeisen-Pensionsfonds-Österreich': 59, 'VT': 60, 'Asian': 61, 'B': 62, 'Carthesio': 63, 'Multipartner': 64, 'SICAV': 65, '(1)': 66, 'Conservative': 67, 'HSBC': 68, 'Portfolio': 69, 'Solutions': 70, 'Strategic': 71, 'Wealth': 72, 'X': 73, 'A': 74, 'American': 75, 'Beacon': 76, 'Class': 77, 'Flexible': 78, 'IHL': 79, 'AXA': 80, 'Capitalisation': 81, 'Funds': 82, 'Global': 83, 'M': 84, 'World': 85}\n",
      "输出当前句子中各个单词的索引编号以及出现频率\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
      "[(0, 1), (2, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n",
      "[(15, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)]\n",
      "[(15, 1), (27, 1), (28, 1), (29, 1), (30, 1)]\n",
      "[(31, 1), (32, 1), (33, 1), (34, 1), (35, 1)]\n",
      "[(6, 1), (36, 1), (37, 1), (38, 1), (39, 1)]\n",
      "[(15, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]\n",
      "[(41, 1), (44, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1)]\n",
      "[(41, 1), (44, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1)]\n",
      "[(58, 1), (59, 1), (60, 1)]\n",
      "[(12, 1), (13, 1), (15, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1)]\n",
      "[(15, 1), (55, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1)]\n",
      "[(10, 1), (15, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1)]\n",
      "[(32, 1), (33, 1), (34, 1), (35, 1), (79, 1)]\n",
      "[(13, 1), (29, 1), (55, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(data_train)\n",
    "print('输出每个单词对应的索引编号')\n",
    "print(dictionary.token2id)\n",
    "dictpath = './nlpmodel/corpus.dict'\n",
    "dictionary.save(dictpath)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_train]\n",
    "print('输出当前句子中各个单词的索引编号以及出现频率')\n",
    "for corpu in corpus:\n",
    "    print(corpu)\n",
    "modelpath = './nlpmodel/corpus.mm'\n",
    "corpora.MmCorpus.serialize(modelpath, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下文将演示如何通过TF/IDF模型求语句相似度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "corpus = corpora.MmCorpus(modelpath)\n",
    "dictionary = corpora.Dictionary.load(dictpath)\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "index = similarities.SparseMatrixSimilarity(\n",
    "    tfidf_model[corpus],\n",
    "    num_features=len(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备测试语句\n",
      "[(0, 1), (2, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n"
     ]
    }
   ],
   "source": [
    "print('准备测试语句')\n",
    "testtext = 'CR-USD ODDO Mid Cap BHF US'.split()\n",
    "doc_text_vec = dictionary.doc2bow(testtext)\n",
    "print(doc_text_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "直接通过TF/IDF模型获取相似度, 返回数值越大，相似度越高\n",
      "[1.6776148  2.421514   0.         0.         0.         0.\n",
      " 0.28899837 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('直接通过TF/IDF模型获取相似度, 返回数值越大，相似度越高')\n",
    "print(index.get_similarities(doc_text_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.0), (0, 0.6401823)]\n"
     ]
    }
   ],
   "source": [
    "test_simi = index[tfidf_model[doc_text_vec]]\n",
    "test_simi = sorted(enumerate(test_simi), key=lambda item: -item[1])\n",
    "outputlist = [test for test in test_simi if test[1] > 0.2]\n",
    "print(outputlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想看与哪一句最相似，直接使用索引，从语料包拿就可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw sentence:  ODDO BHF US Mid Cap CR-USD\n",
      "ODDO BHF US Mid Cap CR-USD ---------similarity:  1.0\n",
      "ODDO BHF US Mid Cap CI-EUR H ---------similarity:  0.6401823\n"
     ]
    }
   ],
   "source": [
    "print('raw sentence: ', ' '.join(testtext))\n",
    "for output in outputlist:\n",
    "    print(namelist[output[0]],'---------similarity: ', output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Doc2Vector中的TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vector其实与Word2Vector类似，都有语义分析成分，但是索引单位是句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vector的训练集的组成单元是TaggedDocument对象, 如下是官方说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represents a document along with a tag, input document format for class: `gensim.models.doc2vec.Doc2Vec`.\n",
    "\n",
    "A single document, made up of `words` (a list of unicode string tokens) and `tags` (a list of tokens).\n",
    "\n",
    "Tags may be one or more unicode string tokens, but typical practice (which will also be the most memory-efficient) is for the tags list to include a unique integer id as the only tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['may', 'prize', 'winner', 'teacher', 'bomb'], ['0'])\n",
      "TaggedDocument(['production', 'value', 'use', 'cgi', 'digital', 'ink', 'paint', 'make', 'thing', 'look', 'really', 'slick', 'voice', 'fine', 'well', 'problem', 'thing', 'script'], ['1'])\n",
      "TaggedDocument(['got', 'heart', 'right', 'place', 'also', 'wilt', 'awhile'], ['2'])\n",
      "TaggedDocument(['prof', 'movie', 'goodness', 'thing', 'good', 'movie'], ['3'])\n",
      "TaggedDocument(['well', 'go', 'forever'], ['4'])\n",
      "TaggedDocument(['overproduced', 'generally', 'disappointing', 'effort', 'likely', 'rouse', 'rush', 'hour', 'crowd'], ['5'])\n"
     ]
    }
   ],
   "source": [
    "sentencelist = [\n",
    "    'may prize winner teacher bomb',\n",
    "    'production value use cgi digital ink paint make thing look really slick voice fine well problem thing script',\n",
    "    'got heart right place also wilt awhile',\n",
    "    'prof movie goodness thing good movie',\n",
    "    'well go forever',\n",
    "    'overproduced generally disappointing effort likely rouse rush hour crowd']\n",
    "x_train = []\n",
    "for index, sentence in enumerate(sentencelist):\n",
    "    document = TaggedDocument(sentence.split(), tags=['{0}'.format(index)])\n",
    "    print(document)\n",
    "    x_train.append(document)\n",
    "# model_dm = Doc2Vec(x_train, min_count=1, window=3, size=200, sample=1e-3, negative=5, workers=2)\n",
    "# print(model_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Keras中的Tokenizer与pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text.Tokenizer类\n",
    "\n",
    "这个类用来对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示。 \n",
    "init(num_words) 构造函数，传入词典的最大值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 成员函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fit_on_text(texts) 使用一系列文档来生成token词典，texts为list类，每个元素为一个文档。\n",
    "- texts_to_sequences(texts) 将多个文档转换为word下标的向量形式,shape为`[len(texts)，len(text)]` -- (文档数，每条文档的长度)\n",
    "- texts_to_matrix(texts) 将多个文档转换为矩阵表示,shape为`[len(texts),num_words]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 成员变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- document_count 处理的文档数量\n",
    "- word_index 一个dict，保存所有word对应的编号id，从<b>1</b>开始\n",
    "- word_counts 一个dict，保存每个word在所有文档中出现的次数\n",
    "- word_docs 一个dict，保存每个word出现的文档的数量\n",
    "- index_docs 一个dict，保存word的id出现的文档的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencelist = [\n",
    "    'may prize winner teacher bomb',\n",
    "    'production value use cgi digital ink paint make thing look really slick voice fine well problem thing script',\n",
    "    'got heart right place also wilt awhile',\n",
    "    'prof movie goodness thing good movie',\n",
    "    'well go forever',\n",
    "    'overproduced generally disappointing effort likely rouse rush hour crowd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_to_word_sequence的用法与字符串的split用法类似\n",
      "['may', 'prize', 'winner', 'teacher', 'bomb']\n"
     ]
    }
   ],
   "source": [
    "print('text_to_word_sequence的用法与字符串的split用法类似')\n",
    "print(text_to_word_sequence(sentencelist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.word_counts\n",
      "OrderedDict([('may', 1), ('prize', 1), ('winner', 1), ('teacher', 1), ('bomb', 1), ('production', 1), ('value', 1), ('use', 1), ('cgi', 1), ('digital', 1), ('ink', 1), ('paint', 1), ('make', 1), ('thing', 3), ('look', 1), ('really', 1), ('slick', 1), ('voice', 1), ('fine', 1), ('well', 2), ('problem', 1), ('script', 1), ('got', 1), ('heart', 1), ('right', 1), ('place', 1), ('also', 1), ('wilt', 1), ('awhile', 1), ('prof', 1), ('movie', 2), ('goodness', 1), ('good', 1), ('go', 1), ('forever', 1), ('overproduced', 1), ('generally', 1), ('disappointing', 1), ('effort', 1), ('likely', 1), ('rouse', 1), ('rush', 1), ('hour', 1), ('crowd', 1)])\n",
      "\n",
      "tokenizer.word_index\n",
      "{'thing': 1, 'well': 2, 'movie': 3, 'may': 4, 'prize': 5, 'winner': 6, 'teacher': 7, 'bomb': 8, 'production': 9, 'value': 10, 'use': 11, 'cgi': 12, 'digital': 13, 'ink': 14, 'paint': 15, 'make': 16, 'look': 17, 'really': 18, 'slick': 19, 'voice': 20, 'fine': 21, 'problem': 22, 'script': 23, 'got': 24, 'heart': 25, 'right': 26, 'place': 27, 'also': 28, 'wilt': 29, 'awhile': 30, 'prof': 31, 'goodness': 32, 'good': 33, 'go': 34, 'forever': 35, 'overproduced': 36, 'generally': 37, 'disappointing': 38, 'effort': 39, 'likely': 40, 'rouse': 41, 'rush': 42, 'hour': 43, 'crowd': 44}\n",
      "\n",
      "tokenizer.word_docs\n",
      "defaultdict(<class 'int'>, {'teacher': 1, 'bomb': 1, 'winner': 1, 'prize': 1, 'may': 1, 'voice': 1, 'script': 1, 'use': 1, 'well': 2, 'problem': 1, 'fine': 1, 'digital': 1, 'ink': 1, 'paint': 1, 'make': 1, 'look': 1, 'slick': 1, 'cgi': 1, 'value': 1, 'thing': 2, 'production': 1, 'really': 1, 'also': 1, 'heart': 1, 'got': 1, 'right': 1, 'wilt': 1, 'place': 1, 'awhile': 1, 'good': 1, 'goodness': 1, 'movie': 1, 'prof': 1, 'forever': 1, 'go': 1, 'disappointing': 1, 'hour': 1, 'generally': 1, 'crowd': 1, 'likely': 1, 'overproduced': 1, 'rouse': 1, 'rush': 1, 'effort': 1})\n",
      "\n",
      "tokenizer.index_docs\n",
      "defaultdict(<class 'int'>, {7: 1, 8: 1, 6: 1, 5: 1, 4: 1, 20: 1, 23: 1, 11: 1, 2: 2, 22: 1, 21: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 19: 1, 12: 1, 10: 1, 1: 2, 9: 1, 18: 1, 28: 1, 25: 1, 24: 1, 26: 1, 29: 1, 27: 1, 30: 1, 33: 1, 32: 1, 3: 1, 31: 1, 35: 1, 34: 1, 38: 1, 43: 1, 37: 1, 44: 1, 40: 1, 36: 1, 41: 1, 42: 1, 39: 1})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(sentencelist)\n",
    "print('tokenizer.word_counts')\n",
    "print(tokenizer.word_counts)\n",
    "print()\n",
    "print('tokenizer.word_index')\n",
    "print(tokenizer.word_index)\n",
    "print()\n",
    "print('tokenizer.word_docs')\n",
    "print(tokenizer.word_docs)\n",
    "\n",
    "print()\n",
    "print('tokenizer.index_docs')\n",
    "print(tokenizer.index_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 1, 17, 18, 19, 20, 21, 2, 22, 1, 23], [24, 25, 26, 27, 28, 29, 30], [31, 3, 32, 1, 33, 3], [2, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentencelist)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One_Hot化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_matrix(sentencelist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad_sequences非常重要，目的是将序列填充到maxlen长度，不足maxlenth的句子，用0填充\n",
    "\n",
    "<b><font color='red'>这个非常重要，Keras用于做分类训练的样本，需要通过填充对齐，才能进行之后的训练</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  5  6  7  8]\n",
      " [ 0  0  9 10 11 12 13 14 15 16  1 17 18 19 20 21  2 22  1 23]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 24 25 26 27 28 29 30]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 31  3 32  1 33  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2 34 35]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 36 37 38 39 40 41 42 43 44]]\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(sequences, maxlen=20)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本清洗，词袋化或“向量化”（这里向量化打引号，表示与真正的词向量概率不同，这里仅仅是将词或句建立向量索引）之后，就是建模了。\n",
    "\n",
    "上述部分已经提及了如何创建TF/IDF这种简单模型，那么如何创建词向量模型(word2vec)，句向量(doc2vec)以及通过Keras创建LSTM, biLSTM, GRU乃至biGRU模型呢？\n",
    "\n",
    "我们将在`3. 常用模型`这一章详细了解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 使用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将在`3. 常用模型`这一章详细了解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 常用自然语言处理包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工欲善其事，必先利其器。目前为止已经有很多很多用于NLP专项应用的python包。\n",
    "\n",
    "下面将逐一介绍它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 JIEBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 常用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 TF/ IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 文档向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 深度学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 样本准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
