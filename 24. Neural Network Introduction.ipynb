{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 以Keras为例的神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 神经网络的基础：感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机在机器学习中有举足轻重的地位，它是SVM和神经网络的基础。\n",
    "\n",
    "总的来说，感知机是机器学习、数据挖掘、神经网络、深度学习等当前计算机行业热门研究方向的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知器的最初概念可以追溯到Warren McCulloch和Walter Pitts在1943年的研究Warren McCulloch和Walter Pitts在1943年的研究，他们将生物神经元类比成带有二值输出的简单逻辑门。\n",
    "\n",
    "以更直观的方式来看，神经元可被理解为生物大脑中神经网络的子节点。\n",
    "\n",
    "在这里，变量信号抵达树突。输入信号在神经细胞体内聚集，当聚集的信号强度超过一定的阈值，就会产生一个输出信号，并被树突传递下去。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj6.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后来，Frank Rosenblatt 1957年发表了一篇论文《The perceptron, a perceiving and recognizing automaton Project Para》。这篇文章最大的贡献就是基于神经元定义了感知机算法的概念。并说明了，感知机算法的目的，就是针对多维特征的样本集合，学习一个权值向量W，使得W乘以输入特征向量X之后，基于乘积，可以判断一个神经元是否被激活。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据感知器的分类示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从机器学习的角度来讲，感知器属于监督学习（supervised learning），它是一个单层的二分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知器是如何工作的呢？如图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj3.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本例中的感知机有三个输入: x1,x2,x3 。通常，它可以有更多或者更少的输入。Rosenblatt提出了一种计算输出的简单的规则。他引入了权重（weight）， w1,w2,… ，等实数来表示各个输入对于输出的重要程度。神经元的输出是0还是1，由加权和 ∑<sub>j</sub>w<sub>j</sub>x<sub>j</sub> 是否小于或者大于某一个阈值（threshold value）。和权重一样，阈值也是一个实数，同时它是神经元的一个参数。使用更严密的代数形式来表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj5.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是感知机的工作方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 广义线性模型下的感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机模型是一个二分类模型，和线性回归模型一样，感知机模型一般会使用一些特征函数φ(x)，将输入空间映射到新的特征空间中，再进行计算。\n",
    "\n",
    "感知机模型对应于特征空间中将实例划分为正负两类的分离超平面，故而是判别式模型。感知机模型的数学表达式如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj8.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其工作模型如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj9.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，神经网络的输入层，隐藏层，连接层以及输出层就有雏形了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里f(.)叫做激活函数(Activation function)，也可以理解为f(x) = sign(w*x+b)。\n",
    "\n",
    "在神经网络中，激活函数，是非线性函数，例如："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj10.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以理解为+1为正例，-1为负例，即激活函数达到输出结果的目的，即二分类。也有正例是1，负例是0的做法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个激活函数，也被称为单位阶跃函数:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj11.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 感知机的缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机是线性的模型，其不能表达复杂的函数，不能出来线性不可分的问题，其连异或问题(XOR）都无法解决，因为异或问题是线性不可分的，怎样解决这个问题呢，通常有两种做法。 \n",
    "\n",
    "其一：用更多的感知机去进行学习，这也就是人工神经网络的由来。 \n",
    "\n",
    "其二：用非线性模型，核技巧，如SVM进行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "附注：通过与门，与非门，或门，实现异或门的图示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj12.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzjxor.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者更直观通过下图表示，通过多个感知机解决线性不可分问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/gzj13.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "典型的神经网络图示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netro1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现一个神经网络有很多小的感知机组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netro2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z为一个简单的线性分类器，g(z)为对其加上激活函数(Active function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数是为了解决神经网络线性不可分的问题，神经网络用于实现复杂的函数，非线性激活函数可以使神经网络随意逼近复杂函数。\n",
    "\n",
    "没有激活函数带来的非线性，多层神经网络和单层无异。\n",
    "\n",
    "其特性如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>非线性</b>： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=xf(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。\n",
    "\n",
    "- <b>可微性</b>： 当优化方法是基于梯度的时候，这个性质是必须的。\n",
    "- <b>单调性</b>： 当激活函数是单调的时候，单层网络能够保证是凸函数。\n",
    "- <b>f(x)≈x</b>： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。\n",
    "- <b>输出值的范围</b>： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数也叫传递函数，有许多种类如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netro3.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图从左至右，从上到下分别为sigmoid,tanh,relu,leaky，其中在人工神经网络中常用的是sigmoid函数，卷积神经网络中常用的是relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid: <img src='./image/sigmoid.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh(双曲正切)：<img src='./image/tanh.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu(线性整流函数):又称修正线性单元, 是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其数学公式为：f(x)=max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky Relu: 其数学公式为：\n",
    "\n",
    "f(x)=max(0.1x, x) \n",
    "\n",
    "Leaky ReLU 的概念是：当 x < 0 时，它得到 0.1 的正梯度。\n",
    "\n",
    "该函数一定程度上缓解了 dead ReLU 问题，但是使用该函数的结果并不连贯。\n",
    "\n",
    "尽管它具备 ReLU 激活函数的所有特征，如计算高效、快速收敛、在正区域内不会饱和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么需要激活函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单来说有两个原因：\n",
    "\n",
    "> - 是否允许当前的信号传递过去，或者以多大的信号传递过去\n",
    "- 使其变成非线性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们不能将所有的信号都传递到下一层，因此要有选择的进行传递，即激励函数可以做到这一点。\n",
    "\n",
    "同时如果不加激励函数，直接让信号过去，那么相当于线性分类器，并没有改变分类器的本质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络优化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化算法的功能，是通过改善训练方式，来<b>最小化(或最大化)损失函数E(x)</b>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型内部有些参数，是用来计算测试集中目标值Y的真实值和预测值的偏差程度的，基于这些参数，就形成了损失函数E(x)。\n",
    "\n",
    "比如说，权重(W)和偏差(b)就是这样的内部参数，一般用于计算输出值，在训练神经网络模型时起到主要作用。\n",
    "\n",
    "在有效地训练模型并产生准确结果时，模型的内部参数起到了非常重要的作用。\n",
    "\n",
    "这也是为什么我们应该用各种优化策略和算法，来更新和计算影响模型训练和模型输出的网络参数，使其逼近或达到最优值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化算法分为两大类："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 一阶优化算法\n",
    "\n",
    "这种算法使用各参数的梯度值来最小化或最大化损失函数E(x)。最常用的一阶优化算法是梯度下降。\n",
    "\n",
    "函数梯度：导数dy/dx的多变量表达式，用来表示y相对于x的瞬时变化率。往往为了计算多变量函数的导数时，会用梯度取代导数，并使用偏导数来计算梯度。梯度和导数之间的一个主要区别是函数的梯度形成了一个向量场。\n",
    "\n",
    "因此，对单变量函数，使用导数来分析；而梯度是基于多变量函数而产生的。更多理论细节在这里不再进行详细解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应一阶偏导数的矩阵，成为Jacobian矩阵（雅克比矩阵）:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/Jacobian.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 二阶优化算法\n",
    "\n",
    "二阶优化算法使用了二阶导数(也叫做Hessian方法)来最小化或最大化损失函数。\n",
    "\n",
    "由于二阶导数的计算成本很高，所以这种方法并没有广泛使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应二阶偏导数的矩阵，称为Hessian矩阵："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/hessian.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化算法用于优化学习速率，让网络用最快的训练次数达到最优，还能防止过拟合。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 常用神经网络优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练和优化智能系统时，梯度下降是一种最重要的技术和基础。梯度下降的功能是：\n",
    "\n",
    ">通过寻找最小值，控制方差，更新模型参数，最终使模型收敛\n",
    "\n",
    "网络更新参数的公式为：θ=θ−η×∇(θ).J(θ)，其中η是学习率，∇(θ).J(θ)是损失函数J(θ)对参数θ的梯度。\n",
    "\n",
    ">∇的念法:Nabla, 微分算子，又称为哈密顿算子。<img src='./image/nubla.svg' />\n",
    "i,j,k是沿x,y,z轴正方向的单位向量\n",
    "\n",
    "这是在神经网络中最常用的优化算法。\n",
    "\n",
    "如今，梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。\n",
    "\n",
    "2006年引入的反向传播技术，使得训练深层神经网络成为可能。\n",
    "\n",
    "反向传播技术是先在前向传播中计算输入信号的乘积及其对应的权重，然后将激活函数作用于这些乘积的总和。\n",
    "\n",
    "这种将输入信号转换为输出信号的方式，是一种对复杂非线性函数进行建模的重要手段，并引入了非线性激活函数，使得模型能够学习到几乎任意形式的函数映射。\n",
    "\n",
    "然后，在网络的反向传播过程中回传相关误差，使用<b>梯度下降</b>更新权重值，通过计算误差函数E相对于权重参数W的梯度，在损失函数梯度的相反方向上更新权重参数。\n",
    "\n",
    "<img src='./image/wg.png' />\n",
    "\n",
    "如图：权重更新方向与梯度方向相反\n",
    "\n",
    "这显示了权重更新过程与梯度矢量误差的方向相反，其中U形曲线为梯度。\n",
    "\n",
    "要注意到，当权重值W太小或太大时，会存在较大的误差，需要更新和优化权重，使其转化为合适值，所以我们试图在与梯度相反的方向找到一个局部最优值。\n",
    "\n",
    "反向传播相关内容，将在后续内容详细阐述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 梯度下降的变体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统的批量梯度下降将计算整个数据集梯度，但只会进行一次更新，因此在处理大型数据集时速度很慢且难以控制，甚至导致内存溢出。\n",
    "\n",
    "权重更新的快慢是由学习率η决定的，并且可以在凸面误差曲面中收敛到全局最优值，在非凸曲面中可能趋于局部最优值。\n",
    "\n",
    "使用标准形式的批量梯度下降还有一个问题，就是在训练大型数据集时存在冗余的权重更新。\n",
    "\n",
    "标准梯度下降的上述问题在随机梯度下降方法中得到了解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. 随机梯度下降(SGD)\n",
    "\n",
    "随机梯度下降（Stochastic gradient descent，SGD）对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快。\n",
    "\n",
    "θ=θ−η⋅∇(θ) × J(θ;x(i);y(i))，其中x(i)和y(i)为训练样本。\n",
    "\n",
    "频繁的更新使得参数间具有高方差，损失函数会以不同的强度波动。\n",
    "\n",
    "这实际上是一件好事，因为它有助于我们发现新的和可能更优的局部最小值，而标准梯度下降将只会收敛到某个局部最优值。\n",
    "\n",
    "但SGD的问题是，由于频繁的更新和波动，最终将收敛到最小限度，并会因波动频繁存在超调量。\n",
    "\n",
    "虽然已经表明，当缓慢降低学习率η时，标准梯度下降的收敛模式与SGD的模式相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. 小批量梯度下降(Mini Batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了避免SGD和标准梯度下降中存在的问题，一个改进方法为小批量梯度下降（Mini Batch Gradient Descent），因为对每个批次中的n个训练样本，这种方法只执行一次更新。\n",
    "\n",
    "使用小批量梯度下降的优点是：\n",
    "\n",
    "1) 可以减少参数更新的波动，最终得到效果更好和更稳定的收敛。\n",
    "\n",
    "2) 还可以使用最新的深层学习库中通用的矩阵优化方法，使计算小批量数据的梯度更加高效。\n",
    "\n",
    "3) 通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。\n",
    "\n",
    "4) 在训练神经网络时，通常都会选择小批量梯度下降算法。\n",
    "\n",
    "这种方法有时候还是被称为SGD。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 使用梯度下降及其变体时面临的挑战\n",
    "\n",
    "1. 很难选择出合适的学习率。太小的学习率会导致网络收敛过于缓慢，而学习率太大可能会影响收敛，并导致损失函数在最小值上波动，甚至出现梯度发散。\n",
    "\n",
    "\n",
    "2. 此外，相同的学习率并不适用于所有的参数更新。如果训练集数据很稀疏，且特征频率非常不同，则不应该将其全部更新到相同的程度，但是对于很少出现的特征，应使用更大的更新率。\n",
    "\n",
    "\n",
    "3. 在神经网络中，最小化非凸误差函数的另一个关键挑战是避免陷于多个其他局部最小值中。实际上，问题并非源于局部极小值，而是来自鞍点，即一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得SGD算法很难脱离出来，因为梯度在所有维度上接近于零。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 进一步优化梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. 动量（Momentum）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述SGD和MBGD算法都存在样本选择的随机性，因此含有较多的噪声。\n",
    "\n",
    "而momentum能解决上述噪声问题，尤其在面对小而较多噪声的梯度时，它往往能加速学习速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/sgdm.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动量（Momentum）的技术，通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练。\n",
    "\n",
    "换句话说，这种新方法将上个步骤中更新向量的分量’γ’添加到当前更新向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/momentum.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动量项γ通常设定为0.9，或相近的某个值。\n",
    "\n",
    "这里的动量与经典物理学中的动量是一致的，就像从山上投出一个球，在下落过程中收集动量，小球的速度不断增加。\n",
    "\n",
    "在参数更新过程中，其原理类似：\n",
    "\n",
    "1) 使网络能更优和更稳定的收敛；\n",
    "\n",
    "2) 减少振荡过程。\n",
    "\n",
    "当其梯度指向实际移动方向时，动量项γ增大；当梯度与实际移动方向相反时，γ减小。\n",
    "\n",
    "这种方式意味着动量项只对相关样本进行参数更新，减少了不必要的参数更新，从而得到更快且稳定的收敛，也减少了振荡过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "motentum方法的缺点主要在于，下坡的过程中动量越来越大，在最低点的速度太大了，可能又冲上坡导致错过极小点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Nesterov梯度加速法 (Nesterov accelerated gradient (NAG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是对motentum算法进行的改进。\n",
    "\n",
    "一位名叫Yurii Nesterov研究员，认为动量方法存在一个问题：\n",
    "\n",
    "如果一个滚下山坡的球，盲目沿着斜坡下滑，这是非常不合适的。\n",
    "\n",
    "一个更聪明的球应该要注意到它将要去哪，因此在上坡再次向上倾斜时小球应该进行减速。\n",
    "\n",
    "实际上，当小球达到曲线上的最低点时，动量相当高。\n",
    "\n",
    "由于高动量可能会导致其完全地错过最小值，因此小球不知道何时进行减速，故继续向上移动。\n",
    "\n",
    "Yurii Nesterov在1983年发表了一篇关于解决动量问题的论文，因此，我们把这种方法叫做Nestrov梯度加速法。\n",
    "\n",
    "在该方法中，他提出先根据之前的动量进行大步跳跃，然后计算梯度进行校正，从而实现参数更新。\n",
    "\n",
    "这种预更新方法能防止大幅振荡，不会错过最小值，并对参数更新更加敏感。\n",
    "\n",
    "给算法增加了预见能力，事先估计出下一个参数处的梯度，用于对当前计算的梯度进行校正。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/nag.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们通过使网络更新与误差函数的斜率相适应，并依次加速SGD，也可根据每个参数的重要性来调整和更新对应参数，以执行更大或更小的更新幅度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Adagrad方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad方法是通过参数来调整合适的学习率η，对稀疏参数进行大幅更新和对频繁参数进行小幅更新。\n",
    "\n",
    "因此，Adagrad方法非常适合处理稀疏数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在时间步长中，Adagrad方法基于每个参数计算的过往梯度，为不同参数θ设置不同的学习率。\n",
    "\n",
    "先前，每个参数θ(i)使用相同的学习率，每次会对所有参数θ进行更新。\n",
    "\n",
    "在每个时间步t中，Adagrad通过分母项来达到不同参数具有不同学习率的目的：，更新对应参数，然后进行向量化。\n",
    "\n",
    "为了简单起见，我们把在t时刻参数θ(i)的损失函数梯度设为g(t,i)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/adagrad.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中G<sub>t</sub>是一个对角阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad方法的主要好处是，不需要手工来调整学习率。大多数参数使用了默认值0.01，且保持不变。\n",
    "\n",
    "Adagrad方法的主要缺点是，学习率η总是在降低和衰减。\n",
    "\n",
    "因为每个附加项都是正的，在分母中累积了多个平方梯度值，故累积的总和在训练期间保持增长。这反过来又导致学习率下降，变为很小数量级的数字，该模型完全停止学习，停止获取新的额外知识。\n",
    "\n",
    "因为随着学习速度的越来越小，模型的学习能力迅速降低，而且收敛速度非常慢，需要很长的训练和学习，即学习速度降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. AdaDelta方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaDelta将分母改进为固定时间长度内的梯度平方和。\n",
    "\n",
    "实际操作中，为了使分母可以收敛（不至于无限增大），采用一个dicount factor求出<b>平均的梯度平方和</b>。\n",
    "\n",
    "与之前无效地存储w先前的平方梯度不同，梯度的和被递归地定义为所有先前平方梯度的衰减平均值。\n",
    "\n",
    "作为与动量项相似的分数γ，在t时刻的滑动平均值Eg²仅仅取决于先前的平均值和当前梯度值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eg²=γ.Eg²+(1−γ).g²(t)，其中γ设置为与动量项相近的值，约为0.9。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/adadelta_1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于分母是梯度平方和的均值的平方根（root mean squared——RMS），因此可以写作： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/adadelta_2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaDelta方法的另一个优点是，已经不需要设置一个默认的学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp是由Hinton发明的，跟AdaDelta基本一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rmsprop.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常将η设为0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 目前已完成的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 为每个参数计算出不同学习率；\n",
    "\n",
    "2) 也计算了动量项momentum；\n",
    "\n",
    "3) 防止学习率衰减或梯度消失等问题的出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 还可以做什么改进？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前的方法中计算了每个参数的对应学习率，但是为什么不计算每个参数的对应动量变化并独立存储呢？\n",
    "\n",
    "这就是<b>Adam</b>算法提出的改良点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. Adam(Adaptive Moment Estimation)算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam算法即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的自适应学习率。\n",
    "\n",
    "这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，或者说Adam不仅利用梯度平方和，也利用梯度的和。\n",
    "\n",
    "这一点与动量类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个公式通过衰减系数β1计算了梯度的第一时刻平均值。\n",
    "\n",
    "第二个公式利用衰减系数β2 计算梯度的第二时刻非中心方差值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/adam.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常将m<sub>t</sub>和v<sub>t</sub>都初始化为零向量。\n",
    "\n",
    "由于β1 and β2都是接近于1的衰减系数，m<sub>t</sub>和v<sub>t</sub>刚开始总是会接近于0。\n",
    "\n",
    "为了解决这个问题，利用下面的方式对m<sub>t</sub>和v<sub>t</sub>进行改进："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/adam_1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">由于衰减系数接近1，分母1−βt1是一个接近0的小数，新的m^t就会在mt的基础上放大好多倍，也就不再容易趋于0了。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后公式更新为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/adam_2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，β1设为0.9，β2设为0.9999，ϵ设为10-8。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 如何选择优化算法？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 如果你的数据很稀疏，那应该选择有自适应性的优化函数。这样你就不用对学习速率进行调优，因为你的数据本来就小，神经网络学习耗时也小。这种情况你更应该关心网络分类的准确率。\n",
    "- RMSprop, Adadelta 和 Adam 非常相似，在相同的情况下表现都很好。\n",
    "- 偏置校验让Adam的效果稍微比RMSprop好一点\n",
    "- 进行过很好的参数调优的SGD+Momentum算法效果好于Adagrad/Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以对不同的参数方案和他们如何快速优化有一个直观的认识： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/optimizer.gif' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别观察一下SGD，红色的那条线，从图中可以看出SGD实际上是所有方法中最慢的一个，所以在实际中很少应用它，我们可以使用更好的方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>如果你不知道为你的神经网络选择哪种优化算法，就直接选Adam吧！</font><b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络过拟合问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的表达能力是非常强大的，只有给予足够多的神经元，通常其都面临着过拟合的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">名词解释\n",
    "<br><b>过拟合(over-fitting)</b>: <br>所谓过拟合（over-fitting）其实就是所建的机器学习模型或者是深度学习模型在训练样本中表现得过于优越，导致在验证数据集以及测试数据集中表现不佳。\n",
    "<br>打个比喻就是当我需要建立好一个模型之后，比如是识别一只狗狗的模型，我需要对这个模型进行训练。\n",
    "<br>恰好，我训练样本中的所有训练图片都是二哈，那么经过多次迭代训练之后，模型训练好了，并且在训练集中表现得很好。基本上二哈身上的所有特点都涵括进去，那么问题来了！\n",
    "<br>假如我的测试样本是一只金毛呢？将一只金毛的测试样本放进这个识别狗狗的模型中，很有可能模型最后输出的结果就是金毛不是一条狗（因为这个模型基本上是按照二哈的特征去打造的）。\n",
    "<br>所以这样就造成了模型过拟合，虽然在训练集上表现得很好，但是在测试集中表现得恰好相反，在性能的角度上讲就是协方差过大（variance is large），同样在测试集上的损失函数（cost function）会表现得很大。\n",
    "<br>\n",
    "<br><b>欠拟合(under-fitting): </b>\n",
    "<br>相对过拟合欠拟合还是比较容易理解。\n",
    "<br>还是拿刚才的模型来说，可能二哈被提取的特征比较少，导致训练出来的模型不能很好地匹配，表现得很差，甚至二哈都无法识别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">过拟合与欠拟合的图例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/fittingresult.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netro4.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，过多的隐含层和神经元的节点，会带来过拟合的问题。\n",
    "\n",
    "对于神经网络，参数膨胀原因可能是因为随着网路深度的增加，同时参数也不断增加，并且增加速度、规模都很大。那么可以<b>采取减少神经网络规模（深度）的方法</b>。\n",
    "\n",
    "同常来说<font color='red'><b>不应该通过降低神经网络的参数量来减少过拟合</b></font>，可以用<b>正则化项</b>对其进行惩罚或者通过<font color='red'>dropout</font>进行一部分神经元的随机失活。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/dropout.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实最关键的是：<font color='red'><b>增大训练样本规模</b></font>防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增大训练样本其实是很难的工作，一般来说，我们会先将机器学习/ 神经网络相关的代码先写好。\n",
    "\n",
    "然后将一定范围的文档，根据关键字跑出疑似正例的语句集合，并请Analyst对这些语句做出类别判定并标记tag，作为最初的样本。\n",
    "\n",
    "这个最初样本即可作为训练集建模，先让模型框架跑起来，之后根据测试集的情况，补充或修订训练集。\n",
    "\n",
    "大致的流程如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/modelcircle.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络欠拟合问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。\n",
    "\n",
    "但是如果真的还是存在的话，可以通过增加网络复杂度或者在模型中增加多点特征点，这些都是很好解决欠拟合的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络反向传播算法(Back Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的训练算法：反向传播算法(Back Propagation)分二步进行，即正向传播和反向传播。这两个过程简述如下：\n",
    "\n",
    "1．正向传播\n",
    "\n",
    "输入的样本从输入层经过隐单元一层一层进行处理，传向输出层；在逐层处理的过程中。在输出层把当前输出和期望输出进行比较，如果现行输出不等于期望输出，则进入反向传播过程。\n",
    "\n",
    "2．反向传播\n",
    "\n",
    "反向传播时，把误差信号按原来正向传播的通路反向传回，逐层修改连接权值，以望代价函数趋向最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播算法的核心目的是对于神经网络中的任何weight或bias计算：损失函数C关于它们的偏导数∂C/∂w. 这个式子能够帮助我们知道当我们改变w或b的时候，损失函数C是怎么变化的。\n",
    "\n",
    "虽然计算这个式子可能有一点复杂，但是它提供了一种自然的，直观的解释，所以说反向传播算法并不仅仅是一种快速学习算法，它提供给我们具体的见解，帮助我们理解改变神经网络的参数是如何改变神经网络行为的。所以说反向传播算法是很值得我们去深入学习的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">名词解释\n",
    "- 损失函数、代价函数与目标函数\n",
    "<br>不严格的说，损失函数和代价函数是同一个东西，目标函数是一个与他们相关但更广的概念，对于目标函数来说在有约束条件下的最小化就是损失函数（loss function）\n",
    "<br>把最大化或者最小化的函数称为目标函数\n",
    "<br>把需要最小化的函数称为代价函数或者损失函数，因为我们的优化是最小化代价或者损失。\n",
    "<br>也就是代价函数与损失函数也属于目标函数，有些目标是最大化，那么就不能叫做损失函数或者代价函数了。\n",
    "<br>更直观的定义:\n",
    "<br>Loss Function 是定义在单个样本上的，算的是一个样本的误差。\n",
    "<br>Cost Function 是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。\n",
    "<br>Object Function（目标函数 ）定义为：Cost Function + 正则化项。\n",
    "<br>如图：\n",
    "<br><img src='./image/mlfunction.png' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">目标函数，损失函数，代价函数的探讨\n",
    "<br><img src='./image/mlfunction2.png' />\n",
    "<br><img src='./image/mlfunction3.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 反向传播的直观描述\n",
    "\n",
    "有这么一个神经网络:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netro5.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设每个训练样本为(向量x, 向量t)，其中向量x是训练样本的特征，而向量t是样本的目标值\n",
    "\n",
    "首先，我们根据样本的特征向量x，计算出神经网络中每个隐藏层节点的输出a<sub>i</sub>，以及输出层每个节点的输出y<sub>i</sub>。\n",
    "\n",
    "然后，我们按照下面的方法计算出每个节点的误差项δ<sub>i</sub>：\n",
    "- 对于输出层节点i:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中δ<sub>i</sub>是节点i的误差项，y<sub>i</sub>是节点i的输出值，t<sub>i</sub>是样本对应于节点i的目标值。\n",
    "\n",
    "举个例子，根据上图，对于输出层节点8来说，它的输出值为y<sub>1</sub>，而样本的目标值是t<sub>1</sub>，代入上面的公式得到节点8的误差项应该是δ<sub>8</sub>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于隐藏层节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof3.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，a<sub>i</sub>是节点i的输出值，w<sub>ki</sub>是节点i到它的下一层节点k的连接的权重，δ<sub>k</sub>是节点i的下一层节点k的误差项。例如，对于隐藏层节点4来说，计算方法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof4.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，更新每个连接上的权值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof5.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，w<sub>ji</sub>是节点i到节点j的权重，η(eta)是一个成为学习速率的常数，δ<sub>j</sub>是节点j的误差项，x<sub>ji</sub>是节点i传递给节点j的输入。例如，权重w<sub>84</sub>的更新方法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof6.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似的，权值w<sub>41</sub>的更新方法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof7.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "偏置项的输入值永远为1。例如，节点w<sub>4b</sub>的偏置项应该按照下面的方法计算："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof8.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经介绍了神经网络每个节点误差项的计算和权重更新方法。\n",
    "\n",
    "显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。\n",
    "\n",
    "这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。\n",
    "\n",
    "这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后，我们就可以根据式5来更新所有的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是基本的反向传播算法，并不是很复杂，您弄清楚了么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 反向传播算法的推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播算法其实就是链式求导法则的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：下面的数学公式部分，看看即可，如果感兴趣可以去：[零基础入门深度学习(3) - 神经网络和反向传播算法](https://www.zybuluo.com/hanbingtao/note/476663#an1)深入学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们看4个方程式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof9.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BP1：方程一：输出层的error(计算最后一层神经网络产生的错误) δ<sup>L</sup>\n",
    "- BP2: 方程二：用当前层error表示下一层error(由后往前，计算每一层神经网络产生的错误)\n",
    "- BP3: 方程三：error等价于损失函数C对bias的变化率(计算权重的梯度)\n",
    "- BP4: 方程四：损失函数C对weights的变化率(计算偏置的梯度)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "附注：Hadamard积,数学表示为：s⊙t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">反向传播要基于常见的线性代数运算，像矩阵的加、乘等等。但是有一种运算是不常见的。例如，s和t是两个相同维度的向量，我们使用s⊙t表示两个向量按元素的积（elementwise product），因此 s⊙t 的元素 (s⊙t)<sub>j</sub>=s<sub>j</sub>t<sub>j</sub> 。如："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/vectorpointmul.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这四个方程的证明主要用到<font color='red'>多元函数微分的链式法则</font>，举例证明公式BP1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof10.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP2, BP3, BP4可以根据上面的参考文章学习，或自行推导, 推导过程参见：[反向传播算法（过程及公式推导）](https://blog.csdn.net/u014313009/article/details/51039334)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 反向传播算法梯度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrof11.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从算法的流程也可以明白为什么它被称为反向传播算法，因为我们是从最后一层的δ<sup>L</sup>开始反向计算前面的δ<sup>l</sup>的。\n",
    "\n",
    "因为损失函数是关于神经网络输出的函数，所以为了得到损失函数关于前面层的参数的梯度就需要不断的应用求导链式法则，一层层向前推导得到我们需要的关系的表达式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 反向传播为什么被认为是快速的算法？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播算法的聪明之处在于我们只需要一次正向遍历神经网络和一次反向遍历神经网络，就可以计算出所有的梯度∂C/∂w<sub>j</sub>\n",
    "\n",
    "正向遍历计算a<sup>l</sup><sub>j</sub>和z<sup>l</sup><sub>j</sub>，反向遍历计算各个δ<sup>l</sup><sub>j</sub>，然后经过简单计算就得到了需要的梯度值。\n",
    "\n",
    "所以说虽然从形式上可能会觉得反向传播算法更复杂，但其实它的计算量更少，算法更高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 反向传播算法的应用意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的训练规则是根据激活函数是sigmoid函数、平方和误差、全连接网络、随机梯度下降优化算法。\n",
    "\n",
    "如果激活函数不同、误差计算方式不同、网络连接结构不同、优化算法不同，则具体的训练规则也会不一样。\n",
    "\n",
    "但是无论怎样，训练规则的推导方式都是一样的，应用链式求导法则进行推导即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 揭开神经网络的盖头"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过了这么多准备工作，我们看看神经网络究竟是什么。\n",
    "\n",
    "先看看多层感知机，顾名思义，就是有多个隐含层的感知机，我们看一下多层感知机的结构："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/mp.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层感知机可以摆脱早期离散传输函数的束缚。\n",
    "\n",
    "使用sigmoid或tanh等连续函数模拟神经元对激励的响应。\n",
    "\n",
    "在训练算法上则使用Werbos发明的反向传播BP算法。\n",
    "\n",
    "这货:多层感知机就是我们现在所说的神经网络NN——神经网络听起来不知道比感知机高端到哪里去了！\n",
    "\n",
    "多层感知机解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 神经网络开始阶段的困境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着神经网络层数的加深，优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优。\n",
    "\n",
    "利用有限数据训练的深层网络，性能还不如较浅层网络。\n",
    "\n",
    "同时，另一个不可忽略的问题是随着网络层数增加，“梯度消失”现象更加严重。\n",
    "\n",
    "具体来说，我们常常使用sigmoid作为神经元的输入输出函数。\n",
    "\n",
    "对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。\n",
    "\n",
    "层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 解决困境的思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a. 预训练\n",
    "2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。\n",
    "\n",
    "这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b. 改变激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了克服梯度消失，ReLU、maxout等传输函数代替了 sigmoid，形成了如今 DNN 的基本形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DNN的新困境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/dnn.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图所示，我们看到全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是参数数量的膨胀。\n",
    "\n",
    "假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。\n",
    "\n",
    "此时我们可以引出卷积神经网络CNN的课题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 卷积神经网络(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积或相关计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一  。由于卷积神经网络能够进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决当神经网络的深度、节点数变大，会导致过拟合、参数过多等问题，加入如下思考："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过抽取只依赖图像里小的子区域的局部特征，然后利用这些特征的信息就可以融合到后续处理阶段中，从而检测更高级的特征，最后产生图像整体的信息。\n",
    "- 距离较近的像素的相关性要远大于距离较远像素的相关性。\n",
    "- 对于图像的一个区域有用的局部特征可能对于图像的其他区域也有用，例如感兴趣的物体发生平移的情形。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3.1 特性：局部连接(Sparse Connectivity)与权值共享(Shared Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图是一个很经典的图示，左边是全连接，右边是局部连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnlocal.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此数目巨大的参数几乎难以训练；\n",
    "\n",
    "而采用<b>局部连接</b>，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管减少了几个数量级，但参数数量依然较多。能不能再进一步减少呢？能！方法就是<b>权值共享</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnsw.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体做法是，在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核(也称滤波器)的大小）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这大概就是CNN的一个神奇之处，尽管只有这么少的参数，依旧有出色的性能。\n",
    "\n",
    "但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为Feature Map。\n",
    "\n",
    "如果有100个卷积核，最终的权值参数也仅为100 × 100 = 10^4个而已。另外，偏置参数也是共享的，同一种滤波器共享一个。\n",
    "\n",
    "卷积神经网络的核心思想是：局部感受野(local field)，权值共享以及时间或空间亚采样这三种思想结合起来，获得了某种程度的位移、尺度、形变不变性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3.2 网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图是一个经典的CNN结构，称为LeNet-5网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet5.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，CNN中主要有两种类型的网络层，分别是卷积层和池化/采样层(Pooling)。\n",
    "\n",
    "卷积层的作用是提取图像的各种特征；\n",
    "\n",
    "池化层的作用是对原始特征信号进行抽象，从而大幅度减少训练参数，另外还可以减轻模型过拟合的程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3.3 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.3.3.1 卷积层(Convolutional Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层的方式对于图像数据来说似乎显得不这么友好，因为图像本身具有“二维空间特征”，通俗点说就是局部特性。\n",
    "\n",
    "譬如我们看一张猫的图片，可能看到猫的眼镜或者嘴巴就知道这是张猫片，而不需要说每个部分都看完了才知道，啊，原来这个是猫啊。\n",
    "\n",
    "所以如果我们可以用某种方式对一张图片的某个典型特征识别，那么这张图片的类别也就知道了。这个时候就产生了卷积的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层是卷积核在上一级输入层上通过逐一滑动窗口计算而得，卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值相乘之和，（通常还要再加上一个偏置参数），得到卷积层上的结果。如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnncl.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的动图能够更好地解释卷积过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/convolved.gif' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再详细一些，举一个例子，现在有一个4*4的图像，我们设计两个卷积核，看看运用卷积核后图片会变成什么样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnclcal.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上图可以看到，原始图片是一张灰度图片,每个位置表示的是像素值，0表示白色，1表示黑色，（0，1）区间的数值表示灰色。对于这个4*4的图像，我们采用两个2*2的卷积核来计算。\n",
    "\n",
    "设定步长为1，即每次以2*2的固定窗口往右滑动一个单位。以第一个卷积核filter1为例，计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "feature_map1(1,1) = 1*1 + 0*(-1) + 1*1 + 1*(-1) = 1 \n",
    "feature_map1(1,2) = 0*1 + 1*(-1) + 1*1 + 1*(-1) = -1 \n",
    "feature_map1(3,3) = 1*1 + 0*(-1) + 1*1 + 0*(-1) = 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到这就是最简单的内积公式。feature_map1(1,1)表示在通过第一个卷积核计算完后得到的feature_map的第一行第一列的值，随着卷积核的窗口不断的滑动，我们可以计算出一个3*3的feature_map1;同理可以计算通过第二个卷积核进行卷积运算后的feature_map2，那么这一层卷积操作就完成了。\n",
    "\n",
    "feature_map尺寸计算公式：[ (原图片尺寸 -卷积核尺寸)/ 步长 ] + 1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，同一层的神经元可以共享卷积核，那么对于高位数据的处理将会变得非常简单。\n",
    "\n",
    "并且使用卷积核后图片的尺寸变小，方便后续计算，并且我们不需要手动去选取特征，只用设计好卷积核的尺寸，数量和滑动的步长就可以让它自己去训练了，省时又省力啊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 为什么卷积核有效？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么使用卷积核计算后分类效果要由于普通的神经网络呢？\n",
    "\n",
    "我们仔细来看一下上面计算的结果：\n",
    "\n",
    "通过第一个卷积核计算后的feature_map是一个三维数据，在第三列的绝对值最大，说明原始图片上对应的地方有一条<b>垂直方向的特征</b>，即像素数值变化较大；\n",
    "\n",
    "而通过第二个卷积核计算后，第三列的数值为0，第二行的数值绝对值最大，说明原始图片上对应的地方有一条<b>水平方向的特征</b>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细思考一下，这个时候，我们设计的两个卷积核分别能够提取，或者说检测出原始图片的特定的特征。\n",
    "\n",
    "此时我们其实就可以把卷积核就理解为特征提取器啊！\n",
    "\n",
    "现在就明白了，为什么我们只需要把图片数据灌进去，设计好卷积核的尺寸、数量和滑动的步长就可以让自动提取出图片的某些特征，从而达到分类的效果啊！\n",
    "\n",
    "注：\n",
    "\n",
    "1.此处的卷积运算是两个卷积核大小的矩阵的内积运算，不是矩阵乘法。即相同位置的数字相乘再相加求和。不要弄混淆了。\n",
    "\n",
    "2.卷积核的公式有很多，这只是最简单的一种。我们所说的卷积核在数字信号处理里也叫滤波器，那滤波器的种类就多了，均值滤波器，高斯滤波器，拉普拉斯滤波器等等，不过，不管是什么滤波器，都只是一种数学运算，无非就是计算更复杂一点。\n",
    "\n",
    "3.每一层的卷积核大小和个数可以自己定义，不过一般情况下，根据实验得到的经验来看，会在越靠近输入层的卷积层设定少量的卷积核，越往后，卷积层设定的卷积核数目就越多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.3.3.2 下采样(池化)层(Pooling Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。\n",
    "\n",
    "为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行池化/采样(Pooling)处理。\n",
    "\n",
    "池化规模一般为2x2\n",
    "\n",
    "池化层的主要目的是通过降采样的方式，在不影响图像质量的情况下，压缩图片，减少参数。\n",
    "\n",
    "池化/采样的方式通常有以下几种：\n",
    "\n",
    "- Max-Pooling: 选择Pooling窗口中的最大值作为采样值；\n",
    "- Mean-ooling: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值；\n",
    "- 高斯池化。借鉴高斯模糊的方法\n",
    "- 可训练池化。训练函数 ff ，接受4个点为输入，输出1个点\n",
    "\n",
    "如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnpool.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算示例: <img src='./image/cnnpool2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单来说，假设现在设定池化层采用MaxPooling，大小为2*2，步长为1，取每个窗口最大的数值重新，那么图片的尺寸就会由3*3变为2*2：(3-2)+1=2。从上例来看，会有如下变换："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算示例: <img src='./image/cnnpool3.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 为什么采用Max Pooling？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从计算方式来看，算是最简单的一种了，取max即可，但是这也引发一个思考，为什么需要Max Pooling，意义在哪里？\n",
    "\n",
    "如果我们只取最大值，那其他的值被舍弃难道就没有影响吗？不会损失这部分信息吗？\n",
    "\n",
    "如果认为这些信息是可损失的，那么是否意味着我们在进行卷积操作后仍然产生了一些不必要的冗余信息呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实从上文分析卷积核为什么有效的原因来看，每一个卷积核可以看做一个特征提取器，不同的卷积核负责提取不同的特征，我们例子中设计的第一个卷积核能够提取出“垂直”方向的特征，第二个卷积核能够提取出“水平”方向的特征，那么我们对其进行<b>Max Pooling操作后，提取出的是真正能够识别特征的数值</b>，其余被舍弃的数值，对于我提取特定的特征并没有特别大的帮助。\n",
    "\n",
    "那么在进行后续计算使，减小了feature map的尺寸，从而减少参数，达到减小计算量，且不损失效果的情况。\n",
    "\n",
    "不过并不是所有情况Max Pooling的效果都很好，有时候有些周边信息也会对某个特定特征的识别产生一定效果，那么这个时候舍弃这部分“不重要”的信息，就不划算了。\n",
    "\n",
    "所以具体情况得具体分析，如果加了Max Pooling后效果反而变差了，不如把卷积后不加Max Pooling的结果与卷积后加了Max Pooling的结果输出对比一下，看看Max Pooling是否对卷积核提取特征起了反效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.3.3.3 Flattern层与全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这一步，其实我们的一个完整的前向传播的“卷积部分”就算完成了，如果想要叠加层数，一般也是叠加“Conv-MaxPooing\",通过不断的设计卷积核的尺寸，数量，提取更多的特征，最后识别不同类别的物体。\n",
    "\n",
    "做完Max Pooling后，我们就会把这些数据“拍平”，丢到Flatten层，然后把Flatten层的output放到full connected Layer里，采用softmax对其进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnfc.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3.4 后向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.3.4.1 卷积层(Convolutional Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当一个卷积层L的下一层(L+1)为采样层，并假设我们已经计算得到了采样层的残差，现在计算该卷积层的残差。\n",
    "\n",
    "从LeNet-5网络结构图我们知道，采样层（L+1）的map大小是卷积层L的1/（scale*scale），以scale=2为例，但这两层的map个数是一样的，卷积层L的某个map中的4个单元与L+1层对应map的一个单元关联，可以对采样层的残差与一个scale*scale的全1矩阵进行[克罗内克积](https://baike.baidu.com/item/%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%E7%A7%AF/6282573?fr=aladdin) 进行扩充，使得采样层的残差的维度与上一层的输出map的维度一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "克罗内克积示例：<img src='./image/hadamard.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "扩展过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnbcl.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用卷积计算卷积层的残差："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnbcl2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.3.4.2 下采样(池化)层(Pooling Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当某个采样层L的下一层是卷积层(L+1)，并假设我们已经计算出L+1层的残差，现在计算L层的残差。\n",
    "\n",
    "采样层到卷积层直接的连接是有权重和偏置参数的，因此不像卷积层到采样层那样简单。\n",
    "\n",
    "现再假设L层第j个map Mj与L+1层的M2j关联，按照BP的原理，L层的残差Dj是L+1层残差D2j的加权和，但是这里的困难在于，我们很难理清M2j的那些单元通过哪些权重与Mj的哪些单元关联，这里需要两个小的变换（rot180°和padding）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rot180°：旋转：表示对矩阵进行180度旋转（可通过行对称交换和列对称交换完成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fz(a):\n",
    "    return a[::-1]\n",
    "\n",
    "def rot180(mat):\n",
    "    return np.array(fz(list(map(fz, mat))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "[[15 14 13 12]\n",
      " [11 10  9  8]\n",
      " [ 7  6  5  4]\n",
      " [ 3  2  1  0]]\n"
     ]
    }
   ],
   "source": [
    "A = np.arange(16).reshape((4,4))\n",
    "B = rot180(A)\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero padding:扩充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的图片由4\\*4，通过卷积层变为3\\*3，再通过池化层变化2\\*2，如果我们再添加层，那么图片岂不是会越变越小？\n",
    "\n",
    "这个时候我们就会引出“Zero Padding”（补零），它可以帮助我们保证每次经过卷积或池化输出后图片的大小不变，如，上述例子我们如果加入Zero Padding，再采用3\\*3的卷积核，那么变换后的图片尺寸与原图片尺寸相同，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnpadding.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常情况下，我们希望图片做完卷积操作后保持图片大小不变，所以我们一般会选择尺寸为3\\*3的卷积核和1的zero padding，或者5\\*5的卷积核与2的zero padding。\n",
    "\n",
    "这样通过计算后，可以保留图片的原始尺寸。\n",
    "\n",
    "那么加入zero padding后的feature_map尺寸 =( width + 2 * padding_size - filter_size )/stride + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如: 如果原矩阵为4x4，则width为3， 如果需要加1个padding size，卷积核为3x3，则filter_size为3，stride为1，则：\n",
    "\n",
    "feature_map size = (4+ 2*1 - 3)/1 + 1 = 4, 即feature_map为4x4的方阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding的实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(in_data, size):\n",
    "    cur_r, cur_w = in_data.shape[0], in_data.shape[1]\n",
    "    new_r = cur_r + size * 2\n",
    "    new_w = cur_w + size * 2\n",
    "    ret = np.zeros((new_r, new_w))\n",
    "    ret[size:cur_r + size, size:cur_w+size] = in_data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "in_data = np.ones((4,4))\n",
    "print(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(padding(in_data, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "旋转180度与padding的综合图例，如："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnrp.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3.4 LeNet-5网络详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上较详细地介绍了CNN的网络结构和基本原理，下面介绍一个经典的CNN模型：LeNet-5网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5：是Yann LeCun在1998年设计的用于手写数字识别的卷积神经网络，当年美国大多数银行就是用它来识别支票上面的手写数字的，它是早期卷积神经网络中最有代表性的实验系统之一。\n",
    "\n",
    "LenNet-5共有7层（不包括输入层），每层都包含不同数量的训练参数，其中主要有2个卷积层，2个下抽样层（池化层），3个全连接层3种连接方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. C1卷积层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet51.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. S2池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet52.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. C3卷积层\n",
    "\n",
    "输入：S2中所有6个或者几个特征map组合\n",
    "\n",
    "卷积核大小：5\\*5\n",
    "\n",
    "卷积核种类：16\n",
    "\n",
    "输出featureMap大小：10\\*10 (14-5+1)=10\n",
    "\n",
    "C3中的每个特征map是连接到S2中的所有6个或者几个特征map的，表示本层的特征map是上一层提取到的特征map的不同组合\n",
    "\n",
    "存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。\n",
    "\n",
    "则：可训练参数：6\\*(3\\*5\\*5+1)+6\\*(4\\*5\\*5+1)+3\\*(4\\*5\\*5+1)+1\\*(6\\*5\\*5+1)=1516\n",
    "\n",
    "连接数：10\\*10\\*1516=151600\n",
    "\n",
    "详细说明：第一次池化之后是第二次卷积，第二次卷积的输出是C3，16个10x10的特征图，卷积核大小是 5\\*5. 我们知道S2 有6个 14\\*14 的特征图，怎么从6 个特征图得到 16个特征图了？ 这里是通过对S2 的特征图特殊组合计算得到的16个特征图。具体如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet53_1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">```\n",
    "C3的前6个feature map（对应上图第一个红框的6列）与S2层相连的3个feature map相连接（上图第一个红框），后面6个feature map与S2层相连的4个feature map相连接（上图第二个红框），后面3个feature map与S2层部分不相连的4个feature map相连接，最后一个与S2层的所有feature map相连。卷积核大小依然为5*5，所以总共有6*(3*5*5+1)+6*(4*5*5+1)+3*(4*5*5+1)+1*(6*5*5+1)=1516个参数。而图像大小为10*10，所以共有151600个连接。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet53.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet53_2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. S4池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "输入：10*10\n",
    "\n",
    "采样区域：2*2\n",
    "\n",
    "采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid\n",
    "\n",
    "采样种类：16\n",
    "\n",
    "输出featureMap大小：5*5（10/2）\n",
    "\n",
    "神经元数量：5*5*16=400\n",
    "\n",
    "连接数：16*（2*2+1）*5*5=2000\n",
    "\n",
    "S4中每个特征图的大小是C3中特征图大小的1/4\n",
    "\n",
    "详细说明：S4是pooling层，窗口大小仍然是2*2，共计16个feature map，C3层的16个10x10的图分别进行以2x2为单位的池化得到16个5x5的特征图。有5x5x5x16=2000个连接。连接的方式与S2层类似。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet54.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. C5卷积层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "输入：S4层的全部16个单元特征map（与s4全相连）\n",
    "\n",
    "卷积核大小：5*5\n",
    "\n",
    "卷积核种类：120\n",
    "\n",
    "输出featureMap大小：1*1（5-5+1）\n",
    "\n",
    "可训练参数/连接：120*（16*5*5+1）=48120\n",
    "\n",
    "详细说明：C5层是一个卷积层。由于S4层的16个图的大小为5x5，与卷积核的大小相同，所以卷积后形成的图的大小为1x1。这里形成120个卷积结果。每个都与上一层的16个图相连。所以共有(5x5x16+1)x120 = 48120个参数，同样有48120个连接。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet55.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. F6全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "输入：c5 120维向量\n",
    "\n",
    "计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过sigmoid函数输出。\n",
    "\n",
    "可训练参数:84*(120+1)=10164\n",
    "\n",
    "详细说明：6层是全连接层。F6层有84个节点，对应于一个7x12的比特图，-1表示白色，1表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是(120 + 1)x84=10164。\n",
    "```\n",
    "\n",
    "<b>倒数第二层是全连接层，输出 K 维度的向量，其中 K 是网络能够预测的类数量。此向量包含任何图像的每个类进行分类的概率。</b>\n",
    "\n",
    "即<b>卷积网络中倒数第二个全连接层的输出才是最后要提取的特征</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASCII编码图如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet56_1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet56.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. 输出层-全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output层也是全连接层，共有10个节点，分别代表数字0到9，且如果节点i的值为0，则网络识别的结果是数字i。\n",
    "\n",
    "采用的是径向基函数（RBF）的网络连接方式。假设x是上一层的输入，y是RBF的输出，则RBF输出的计算方式是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rbf.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式w_ij 的值由i的比特图编码确定，i从0到9，j取值从0到7\\*12-1。\n",
    "\n",
    "RBF输出的值越接近于0，则越接近于i，即越接近于i的ASCII编码图，表示当前网络输入的识别结果是字符i。\n",
    "\n",
    "该层有84x10=840个参数和连接。\n",
    "\n",
    "<b>即CNN的最后一层使用分类层（比如 softmax）提供分类输出。</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet57.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数字3通过LeNet-5的识别过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lenet57_1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理论部分说的非常非常多了，下面看一下如何用Keras做一个mnist，即手写数字的CNN神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1671)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the convnet \n",
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes):\n",
    "        model = Sequential()\n",
    "        print(input_shape)\n",
    "        # CONV => RELU => POOL\n",
    "        model.add(Conv2D(20, kernel_size=5, padding=\"same\",\n",
    "                         input_shape=input_shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # CONV => RELU => POOL\n",
    "        model.add(Conv2D(50, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # Flatten => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    " \n",
    "        # a softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "OPTIMIZER = Adam()\n",
    "VALIDATION_SPLIT=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "INPUT_SHAPE = (1, IMG_ROWS, IMG_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "K.set_image_dim_ordering(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider them as float and normalize\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255 \n",
    "X_test /= 255  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为训练一次mnist太慢了，为了演示，仅仅训练1000个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a 60K x [1 x 28 x 28] shape as input to the CONVNET\n",
    "X_train = X_train[:, np.newaxis, :, :][:1000]\n",
    "X_test = X_test[:, np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train[:1000], NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 25s 31ms/step - loss: 1.9333 - acc: 0.4088 - val_loss: 1.2568 - val_acc: 0.6400\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 24s 30ms/step - loss: 0.9197 - acc: 0.7338 - val_loss: 0.6622 - val_acc: 0.7600\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 23s 29ms/step - loss: 0.5061 - acc: 0.8387 - val_loss: 0.6154 - val_acc: 0.7950\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 0.4441 - acc: 0.8450 - val_loss: 0.5585 - val_acc: 0.8300\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 0.3281 - acc: 0.8875 - val_loss: 0.4278 - val_acc: 0.8700\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 0.2423 - acc: 0.9288 - val_loss: 0.3879 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 0.1842 - acc: 0.9500 - val_loss: 0.3823 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 0.1329 - acc: 0.9688 - val_loss: 0.3491 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 0.0947 - acc: 0.9725 - val_loss: 0.3418 - val_acc: 0.9100\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 23s 28ms/step - loss: 0.0723 - acc: 0.9850 - val_loss: 0.3558 - val_acc: 0.8950\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 23s 29ms/step - loss: 0.0556 - acc: 0.9838 - val_loss: 0.3468 - val_acc: 0.9050\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 23s 28ms/step - loss: 0.0474 - acc: 0.9925 - val_loss: 0.3439 - val_acc: 0.9050\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 23s 29ms/step - loss: 0.0448 - acc: 0.9875 - val_loss: 0.3992 - val_acc: 0.8950\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 0.0426 - acc: 0.9925 - val_loss: 0.3497 - val_acc: 0.9200\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.3811 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 0.0181 - acc: 0.9975 - val_loss: 0.3391 - val_acc: 0.9250\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 0.0141 - acc: 0.9988 - val_loss: 0.3280 - val_acc: 0.9300\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.3651 - val_acc: 0.9200\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.3652 - val_acc: 0.9250\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.3591 - val_acc: 0.9250\n"
     ]
    }
   ],
   "source": [
    "modelpath = './cnnmodel/mnistcnn.h5'\n",
    "model = None\n",
    "history = None\n",
    "if os.path.exists(modelpath):\n",
    "    try:\n",
    "        K.clear_session()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    model = load_model(modelpath)\n",
    "else:\n",
    "    # initialize the optimizer and model\n",
    "    model = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    history = model.fit(X_train, y_train, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "    model.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 16s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test score: 0.26134073113417255\n",
      "Test accuracy: 0.9336\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xt8lNWd+PHPd3K/AblxDZcEqYqCCIiiVbT1AlTF1ru1tbYutdv+tt1etTe77nbX3W5ba9Va2rLVbdV6rWzFe6VqFRAQAeUeLgkJEBIIkHtmvr8/zpMwhJkwuTyZkPm+X695zTPPOc/MN5NJvnPOeZ5zRFUxxhhjjicQ7wCMMcacGCxhGGOMiYklDGOMMTGxhGGMMSYmljCMMcbExBKGMcaYmFjCMKYXiMjvReTfYqy7XUQu7unzGNPXLGEYY4yJiSUMY4wxMbGEYRKG1xX0LRFZIyJ1IvI7ERkmIi+IyCEReVVEcsPqXykiH4jIARFZIiKnhpWdKSKrvOP+BKR3eK3LRWS1d+zbIjK5mzH/g4hsEZEaEVkkIiO9/SIiPxeRvSJS6/1Mp3tlc0XkQy+2XSLyzW69YcZ0YAnDJJqrgUuAjwBXAC8A3wUKcH8P/wQgIh8BHgO+BhQCi4H/E5FUEUkF/gz8L5AHPOk9L96xU4GFwBeBfODXwCIRSetKoCLyMeA/gOuAEcAO4HGv+FLgAu/nGAJcD1R7Zb8DvqiqOcDpwF+78rrGRGMJwySaX6rqHlXdBbwJLFPV91S1CXgWONOrdz3wvKq+oqotwH8DGcC5wDlACnCvqrao6lPAu2Gv8Q/Ar1V1maoGVfVhoMk7ris+DSxU1VVefHcCM0VkHNAC5ACnAKKq61W10juuBZgoIoNUdb+qruri6xoTkSUMk2j2hG03RHic7W2PxH2jB0BVQ0AZMMor26VHz9y5I2x7LPANrzvqgIgcAEZ7x3VFxxgO41oRo1T1r8D9wAPAHhFZICKDvKpXA3OBHSLyNxGZ2cXXNSYiSxjGRFaB+8cPuDED3D/9XUAlMMrb12ZM2HYZ8GNVHRJ2y1TVx3oYQxaui2sXgKrep6rTgNNwXVPf8va/q6rzgKG4rrMnuvi6xkRkCcOYyJ4APiEiHxeRFOAbuG6lt4F3gFbgn0QkWUQ+BcwIO/Y3wO0icrY3OJ0lIp8QkZwuxvAocKuITPHGP/4d14W2XUTO8p4/BagDGoGgN8byaREZ7HWlHQSCPXgfjGlnCcOYCFR1I3Az8EtgH26A/ApVbVbVZuBTwOeA/bjxjmfCjl2BG8e43yvf4tXtagyvAT8Ansa1asYDN3jFg3CJaT+u26oaN84C8Blgu4gcBG73fg5jekxsASVjjDGxsBaGMcaYmFjCMMYYExNLGMYYY2JiCcMYY0xMkv16YhEZDTwCDAdCwAJV/UWHOgL8AneRUT3wubarUkXkFuD7XtV/866W7VRBQYGOGzeu134GY4wZ6FauXLlPVQtjqetbwsCdp/4NVV3lnX++UkReUdUPw+rMASZ4t7OBXwFni0gecBcwHVDv2EWqur+zFxw3bhwrVqzw42cxxpgBSUR2HL+W41uXlKpWtrUWVPUQsB43rUK4ecAj6iwFhojICOAy4BVVrfGSxCvAbL9iNcYYc3x9MobhTZZ2JrCsQ9Eo3DQKbcq9fdH2R3ru+SKyQkRWVFVV9VbIxhhjOvA9YYhINu5K1a+p6sGOxREO0U72H7tTdYGqTlfV6YWFMXXDGWOM6QY/xzDw5rl5Gvijqj4ToUo5bkK3NkW4CdfKgQs77F/SnRhaWlooLy+nsbGxO4efMNLT0ykqKiIlJSXeoRhjBig/z5IS3EIu61X1Z1GqLQK+IiKP4wa9a1W1UkReAv49bPWzS3FrAXRZeXk5OTk5jBs3jqMnFx04VJXq6mrKy8spLi6OdzjGmAHKzxbGebhJ0NaKyGpv33fxpoFW1Ydwq5jNxU3OVg/c6pXViMi/cmRRmrtVtaY7QTQ2Ng7oZAEgIuTn52NjOMYYP/mWMFT1LSKPRYTXUeDLUcoW4pa57LGBnCzaJMLPaIyJr4S/0ltV2XuwkUONLfEOxRhj+rWETxgAVYebqG3wJ2EcOHCABx98sMvHzZ07lwMHDvgQkTHGdE/CJwwRIS05iabWkC/PHy1hBIOdL4K2ePFihgwZ4ktMxhjTHb6eVnuiSEsOcKip1ZfnvuOOO9i6dStTpkwhJSWF7OxsRowYwerVq/nwww+56qqrKCsro7Gxka9+9avMnz8fODLNyeHDh5kzZw4f/ehHefvttxk1ahTPPfccGRkZvsRrjDHRJFTC+Jf/+4APKzpeOwgtwRDNrSGy0rr+dkwcOYi7rjgtavk999zDunXrWL16NUuWLOETn/gE69ataz/9deHCheTl5dHQ0MBZZ53F1VdfTX5+/lHPsXnzZh577DF+85vfcN111/H0009z88226qYxpm8lVMKIJuCdYRRSbd/2y4wZM466VuK+++7j2WefBaCsrIzNmzcfkzCKi4uZMmUKANOmTWP79u2+xmiMMZEkVMKI1hJobAmyac8hRudmkpuV6msMWVlZ7dtLlizh1Vdf5Z133iEzM5MLL7ww4hXpaWlp7dtJSUk0NDT4GqMxxkSS8IPeAKnJAQTxZeA7JyeHQ4cORSyrra0lNzeXzMxMNmzYwNKlS3v99Y0xprckVAsjmoAIqckBmlo7P3OpO/Lz8znvvPM4/fTTycjIYNiwYe1ls2fP5qGHHmLy5MmcfPLJnHPOOb3++sYY01vEXWw9MEyfPl07LqC0fv16Tj311OMeu31fHc3BEB8ZluNXeL6L9Wc1xpg2IrJSVafHUte6pDxpKQGaWkMMpARqjDG9yRKGJy05gKrSHPTnAj5jjDnRWcLwpCUnAdDUYgnDGGMisYThSUt2b4VfU4QYY8yJzhKGJzkpQFJAfDlTyhhjBgJLGGH8nITQGGNOdJYwwqQlB3p9DKO705sD3HvvvdTX1/dqPMYY012+JQwRWSgie0VkXZTyb4nIau+2TkSCIpLnlW0XkbVe2YpIx/shLSVAayhEMNR7ScMShjFmoPDzSu/fA/cDj0QqVNWfAD8BEJErgH/usG73Raq6z8f4jtF+plRriMzU3sml4dObX3LJJQwdOpQnnniCpqYmPvnJT/Iv//Iv1NXVcd1111FeXk4wGOQHP/gBe/bsoaKigosuuoiCggJef/31XonHGGO6y881vd8QkXExVr8ReMyvWNq9cAfsXhu1OEeVkuYgKSkBCMSYMIZPgjn3RC0On9785Zdf5qmnnmL58uWoKldeeSVvvPEGVVVVjBw5kueffx5wc0wNHjyYn/3sZ7z++usUFBR06cc0xhg/xH0MQ0QygdnA02G7FXhZRFaKyPzjHD9fRFaIyIqqqqoexuK9uE9Xe7/88su8/PLLnHnmmUydOpUNGzawefNmJk2axKuvvsp3vvMd3nzzTQYPHuzL6xtjTE/0h8kHrwD+3qE76jxVrRCRocArIrJBVd+IdLCqLgAWgJtLqtNX6qQlACDArt2HSE8JMDY/q9O63aGq3HnnnXzxi188pmzlypUsXryYO++8k0svvZQf/vCHvf76xhjTE3FvYQA30KE7SlUrvPu9wLPAjL4KJi050Kun1oZPb37ZZZexcOFCDh8+DMCuXbvYu3cvFRUVZGZmcvPNN/PNb36TVatWHXOsMcbEW1xbGCIyGJgF3By2LwsIqOohb/tS4O6+iiktxa3vrapIL6y+Fz69+Zw5c7jpppuYOXMmANnZ2fzhD39gy5YtfOtb3yIQCJCSksKvfvUrAObPn8+cOXMYMWKEDXobY+LOt+nNReQx4EKgANgD3AWkAKjqQ16dzwGzVfWGsONKcK0KcAntUVX9cSyv2ZPpzdvU1DVTvr+ek4fntJ81daKw6c2NMV3VlenN/TxL6sYY6vwed/pt+L5S4Ax/ojq+9jmlWkInXMIwxhg/9YcxjH7FJiE0xpjIEiJhdKXbLTkpQPIJOAmhLfxkjPHbgE8Y6enpVFdXd+kfauoJNgmhqlJdXU16enq8QzHGDGD94ToMXxUVFVFeXk5XLurbX99MY0uQ5n0ZPkbWu9LT0ykqKop3GMaYAWzAJ4yUlBSKi4u7dMxDf9vKPS9s4P27LmVwRopPkRljzIllwHdJdcf4wmwASqsOxzkSY4zpPyxhRFBS6KYF2VpVF+dIjDGm/7CEEcGYvEySA2ItDGOMCWMJI4KUpABj8jPZagnDGGPaWcKIYnxhNqXWJWWMMe0sYURRUpjF9uo6WoMnzvUYxhjjJ0sYUYwvzKYlqJTvb4h3KMYY0y9YwohifPuZUjaOYYwxYAkjqpKCtmsxbBzDGGPAEkZUuVmp5GWlWgvDGGM8ljA6UVKQZS0MY4zxWMLoxPjCbEr3WQvDGGPAx4QhIgtFZK+IrItSfqGI1IrIau/2w7Cy2SKyUUS2iMgdfsV4PCWFWew73ExtfUu8QjDGmH7DzxbG74HZx6nzpqpO8W53A4hIEvAAMAeYCNwoIhN9jDOqtkkIt1orwxhj/EsYqvoGUNONQ2cAW1S1VFWbgceBeb0aXIzaJyHcawnDGGPiPYYxU0TeF5EXROQ0b98ooCysTrm3LyIRmS8iK0RkRVcWSYrF6LxMUpKE0n028G2MMfFMGKuAsap6BvBL4M/efolQN+r6qqq6QFWnq+r0wsLCXg0wJSnAmLxMa2EYYwxxTBiqelBVD3vbi4EUESnAtShGh1UtAiriECLQdqaUtTCMMSZuCUNEhouIeNszvFiqgXeBCSJSLCKpwA3AonjFWVKYzQ6bhNAYY/xb01tEHgMuBApEpBy4C0gBUNWHgGuAL4lIK9AA3KCqCrSKyFeAl4AkYKGqfuBXnMczvjCLlqBStr+B4oKseIVhjDFx51vCUNUbj1N+P3B/lLLFwGI/4uqqkrZTa/cetoRhjElo8T5Lqt9rm7XWrvg2xiQ6SxjHMSQzlfysVJtTyhiT8CxhxKCkMMtmrTXGJDxLGDGw9b2NMcYSRkxKCrOormvmQH1zvEMxxpi4sYQRg/ZJCK2VYYxJYJYwYtB+aq2NYxhjEpgljBiMzs1wkxBaC8MYk8AsYcQgOSnA2Hw7U8oYk9gsYcRofGEWpZYwjDEJzBJGjNwkhPW02CSExpgEZQkjRiUFWbSGlLKa+niHYowxcWEJI0bjh9qptcaYxGYJI0bjC1zCsHEMY0yisoQRo8GZKRRk2ySExpjEZQmjC0oKsu3UWmNMwvItYYjIQhHZKyLropR/WkTWeLe3ReSMsLLtIrJWRFaLyAq/Yuyq8UOzbH1vY0zC8rOF8Xtgdifl24BZqjoZ+FdgQYfyi1R1iqpO9ym+Iw7tgYOVx61WUpBNTV0z++tsEkJjTOLxLWGo6htATSflb6vqfu/hUqDIr1g61dII906CpQ8et+r4obb6njEmcfWXMYwvAC+EPVbgZRFZKSLzOztQROaLyAoRWVFVVdX1V05Jh6KzYNvfjlu1pKBtfW/rljLGJJ64JwwRuQiXML4Ttvs8VZ0KzAG+LCIXRDteVReo6nRVnV5YWNi9IEpmQeUaqI/aIAKgKDeD1KQAW62FYYxJQHFNGCIyGfgtME9Vq9v2q2qFd78XeBaY4WsgxbMAhW1vdFrNTUKYaS0MY0xCilvCEJExwDPAZ1R1U9j+LBHJadsGLgUinmnVa0ZNhdScmLqlxhdm2xiGMSYhJfv1xCLyGHAhUCAi5cBdQAqAqj4E/BDIBx4UEYBW74yoYcCz3r5k4FFVfdGvOAFISoFx50FpDOMYhVm8un4PLcEQKUlx79Ezxpg+41vCUNUbj1N+G3BbhP2lwBnHHuGz4lmw6UU4UAZDRketVlKYTWtI2VlT3750qzHGJAL7itymZJa7P0631PhC79RamyLEGJNgLGG0GToRsgqP2y1l63sbYxKVJYw2Iq5batvfQDVqtcEZKRRkp9mstcaYhGMJI1zJLDi8B6o2dF6tMMvWxTDGJBxLGOGKvXGM43RLjS/MthaGMSbhWMIIlzsWcotjGvjeX99CjU1CaIxJIJYwOiqZBdvfgmBr1Cptp9NaK8MYk0gsYXRUPAuaDkLFe1GrlHin1tqZUsaYRGIJo6Nib57DbUuiVinKzSQ1KWDXYhhjEooljI6yCmD4pE4HvpMCwriCTGthGGMSiiWMSIpnQdkyaK6PWqWkINtaGMaYhGIJI5KSCyHYDGVLo1YZPzSLnTX1tARDfRaWMcbEkyWMSMbMhEByp91SJQVuEsId1dFbIcYYM5BYwogkLRuKZnR6Pcb4oXZqrTEmsVjCiKZkFlSsjrps65FTa20cwxiTGCxhRNO2bOv2tyIWD0pPoTDHJiE0xiQOSxjRFE2H1OxOu6VKCrLs1FpjTMKIKWGIyFdFZJA4vxORVSJyaQzHLRSRvSIScU1u7/nuE5EtIrJGRKaGld0iIpu92y2x/0i9JCkFxp7b6cD3+KHZbK2qQzuZDt0YYwaKWFsYn1fVg8ClQCFwK3BPDMf9HpjdSfkcYIJ3mw/8CkBE8nBrgJ8NzADuEpHcGGPtPcWzoHoz1O6KWFxSkEVtg01CaIxJDLEmDPHu5wL/o6rvh+2LSlXfACKPGjvzgEfUWQoMEZERwGXAK6pao6r7gVfoPPH44zjLtrafKbXPBr6NMQNfrAljpYi8jEsYL4lIDtAbV6yNAsrCHpd7+6LtP4aIzBeRFSKyoqqqqhdCCjP0NMgsiNotNb7AW651r41jGGMGvlgTxheAO4CzVLUeSMF1S/VUpFaKdrL/2J2qC1R1uqpOLyws7IWQwgQCbjLCKMu2jsrNIDU5YC0MY0xCiDVhzAQ2quoBEbkZ+D5Q2wuvXw6MDntcBFR0sr/vlcyCQ5Wwb9MxRUkBoTg/y1oYxpiEEGvC+BVQLyJnAN8GdgCP9MLrLwI+650tdQ5Qq6qVwEvApSKS6w12X+rt63vHWba1pDDLWhjGmIQQa8JoVXfu6DzgF6r6CyDneAeJyGPAO8DJIlIuIl8QkdtF5HavymKgFNgC/Ab4RwBVrQH+FXjXu93t7et7ecUwZGz0ge/CbHbW1NPcapMQGmMGtuQY6x0SkTuBzwDni0gSbhyjU6p643HKFfhylLKFwMIY4/NXySz44Dm3bGvS0W9ZSWEWwZCys6aOk4YeN4caY8wJK9YWxvVAE+56jN24M5Z+4ltU/U3xLGiqhcr3jylqW9/b5pQyxgx0MSUML0n8ERgsIpcDjaraG2MYJ4a2cYwIy7ba+t7GmEQR69Qg1wHLgWuB64BlInKNn4H1K9mFMOz0iAPfOekpjMvP5K3N++IQmDHG9J1Yu6S+h7sG4xZV/Sxuuo4f+BdWP1Q8C3YuhZaGY4o+eWYRb2+tpqzGFlMyxgxcsSaMgKruDXtc3YVjB4aSWRBscmt9d3D1NHcR+jOrIs85ZYwxA0Gs//RfFJGXRORzIvI54HncKbGJY+y5UZdtLcrN5Nzx+Ty1qoxQyGauNcYMTLEOen8LWABMBs4AFqjqd/wMrN9Jy4FR06Nej3Ht9CLKahpYti0+l4sYY4zfYu5WUtWnVfXrqvrPqvqsn0H1WyWzoOI9aDhwTNHs00aQnZbMUyvL4xCYMcb4r9OEISKHRORghNshETnYV0H2G8WzQEMRl23NSE3i8skjWLy2ksNNrXEIzhhj/NVpwlDVHFUdFOGWo6qD+irIfqPoLEjJ7LRbqqElyOI1lX0cmDHG+C+xznTqqeRUb9nWJRGLp47JpaQwiydXlkUsN8aYE5kljK4qnuWmOj947GzrIsI104p4d/t+ttsMtsaYAcYSRle1L9v6RsTiT51ZRECwwW9jzIBjCaOrhk2CjLyo62MMH5zO+RMKeXpVOUG7JsMYM4BYwuiqtmVbS5dEXLYV3OB3ZW0jb2+1+aWMMQOHJYzuKJkFhyqgekvE4otPHcag9GSeXGHdUsaYgcPXhCEis0Vko4hsEZE7IpT/XERWe7dNInIgrCwYVrbIzzi7rH3Z1iURi9NTkpg3ZRQvfbCb2oaWvovLGGN85FvC8FblewCYA0wEbhSRieF1vKvGp6jqFOCXwDNhxQ1tZap6pV9xdkteCQweE/V6DHDdUk2tIf6y5tizqYwx5kTkZwtjBrBFVUtVtRl4HLcmeDQ3Ao/5GE/vEYGSC9yZUqFgxCqTRg3m5GE51i1ljBkw/EwYo4DwK9jKvX3HEJGxQDHw17Dd6SKyQkSWishV/oXZTcUXQmPkZVvhyDUZq8sOsGXvob6NzRhjfOBnwpAI+6KdZ3oD8JSqhn9dH6Oq04GbgHtFZHzEFxGZ7yWWFVVVVT2LuCuKL3D3nXRLXXXmKJICwpN2TYYxZgDwM2GUA6PDHhcB0Tr0b6BDd5SqVnj3pcAS4MxIB6rqAlWdrqrTCwsLexpz7HKGwdCJUa/HACjMSeOik4fyzKpdtAZDfRebMcb4wM+E8S4wQUSKRSQVlxSOOdtJRE4GcoF3wvblikiat10AnAd86GOs3VM8C3a+Ay2NUatcM62IqkNNvLG5D1s/xhjjA98Shqq2Al8BXgLWA0+o6gcicreIhJ/1dCPwuOpRV8GdCqwQkfeB14F7VLX/JYySWdDaCOXLo1b52ClDyctKtalCjDEnvGQ/n1xVF9NhKVdV/WGHxz+KcNzbwCQ/Y+sVY88DSXLdUm1jGh2kJge4asoo/rB0B/vrmsnNSu3jII0xpnfYld49kT4IRk2LegFfm2umFdEcDPHc6l19E5cxxvjAEkZPlcyCilXuFNsoJo4cxGkjB/HUKuuWMsacuCxh9FQny7aGu3ZaEet2HWR9ZeKtbGuMGRgsYfTU6BmQNgievBX+cA28+1uoPbYlMW/KKFKSxK78NsacsCxh9FRyGty6GM76gpu99vlvwM9Pg4c+Cn/9NyhfCaEQuVmpXHzqMP68ehfNrXZNhjHmxCMaZU2HE9H06dN1xYoV8QtA1S3fuulF2PgilC113VXZw2DCpazJOofrX83g3s+cx2WnDY9fnMYY4xGRld6sGsevawnDR/U1sOVV2PgCbHkNmmppIoVNGWcy6WPXw0dmw+CieEdpjElgljD6o2AL7Hib5S89ytDdSxgnu93+YZPg5NkwcR4M7/+XnhhjBhZLGP3Ylr2HuPhnf+MnF2Zwbc4Hrvtq5zuu62roaXDG9TDpWhg0Mt6hGmMSgCWMfu6qB/5OQ3OQF792PiLiuq4+eAbefxzK3wXEXd8x+QY49QpIy453yMaYAaorCcPOkoqDa6cXsXHPIdbu8i72y8yDs26D216F/7cKZn0barbBn2+H/54Az8x3YyBRFmsyxpi+YAkjDi6fPJK05EDkCQnzx8NF34Wvvg+3vgiTr3PdVn/4FPxsIrz0Pdi9tu+DNsYkPEsYcTA4I4XLThvOc6sraGyJ0moQgbEz4YpfwDc2wXWPwKipsOwhd43Hg+fC3++Dg5V9G7wxJmFZwoiTa6cXUdvQwqvr9xy/ckq6O4vqxsdc8pj735CSAa/8AH4+ER65Cna87X/QxpiEZgkjTs4dX8CIweldXycjKx9m/AP8w2vwlZVw/jfcxYIPXwEr/sefYI0xBksYcZMUEK6eWsQbm6rYXRt9xb5OFZwEH/s+/OM7bhLEv3wNXrgDgq29G6wxxmAJI66umVZESOGZ93o4IWH6YLjpCTj7S7DsV/DYDZ1Ot26MMd3ha8IQkdkislFEtojIHRHKPyciVSKy2rvdFlZ2i4hs9m63+BlnvIwryOKscbk8tbKcHl8Pk5QMc+6By38Opa/D7y51p+YaY0wv8S1hiEgS8AAwB5gI3CgiEyNU/ZOqTvFuv/WOzQPuAs4GZgB3iUiuX7HG07XTRlNaVceqnQd65wmnfx5ufgYO7YbfftwGw40xvcbPFsYMYIuqlqpqM/A4MC/GYy8DXlHVGlXdD7wCzPYpzriaO3kEWalJ3Po/y/nRog/YuPtQz5+0ZBbc9hpk5MLDV8J7f+z5cxpjEp6fCWMUUBb2uNzb19HVIrJGRJ4SkdFdPBYRmS8iK0RkRVVVVW/E3aey05J5bP45zDp5KI8u28ll977BJx/8O0+sKKO+uQeD1wUnuSvHx54Lz/0jvPwDu1LcGNMjfiYMibCvY0f9/wHjVHUy8CrwcBeOdTtVF6jqdFWdXlhY2O1g42ly0RB+eeOZLP3ux/n+J07lYEML335qDWf/+DW+/+e1rNvVzQHsjFy4+WmY/gV4+z74083QdLh3gzfGJAw/E0Y5MDrscRFQEV5BVatVtcl7+BtgWqzHDkR5Wancdn4Jr359Fk98cSaXTBzGkyvKufyXb3Hl/W/x6LKdHG7qYqsjKQUu/xnM+YmbYmThZXCg7PjHGWNMB77NVisiycAm4OPALuBd4CZV/SCszghVrfS2Pwl8R1XP8Qa9VwJTvaqrgGmqWtPZa54os9V2RW19C8++V85jy8vYuOcQmalJXHnGSG6cMYbJRYPdbLex2vKaW3s8ORVueNStR26MSWj9ZnpzEZkL3AskAQtV9ccicjewQlUXich/AFcCrUAN8CVV3eAd+3ngu95T/VhVj3sZ80BMGG1UlffKDvD48p383/uVNLQEOXXEIG6cMZp5U0YxOCMltieq2giPXg8HK2De/W5yQ2NMwuo3CaOvDeSEEe5gYwuLVlfw2PKdfFBxkPSUALd9tISvXTyB5KQYehnra+BPn4Edb8H534SLvgcBu4bTmERkCSOBrC2v5bdvlfLc6gqmj83lvhvPZOSQjOMf2NoMz38d3vtfOPVKuOpBSMvxP2BjTL9iCSMBPbd6F999Zi0pyQF+eu0ZfPzUYcc/SBWWPujW2EhKgaIZUHw+jDsfiqZDcpr/gRtj4soSRoLatq+Orzy6ig8qDnLbR4v59uxTSE2Ooaup7F1Y/xxsewMq1wAKyRkw5myXPIpnwcgpLqkYYwYUSxgJrLElyH8sXs/D7+zgjKLB3H/TVEbnZcb+BA373XQi2950CWSvd1JbajaMmelaIMUXwPDJEEjy54cwfatun7uoMyeGVqkZcCxhGF5cV8m3nloDwH9dPZk5k0Z074nq9sH2t1zy2P6mW3sD3Ay5Y8+26OdVAAAWO0lEQVTzWiAXuKVlk9PdSoGm/2s8CBueh7VPQOkS0BCMmganXA6nXgEFE+IdoekjljAMAGU19Xzlsfd4v+wAn505lu/OPZX0lB62Cg7tdq2P7W+4+/0dZsRNTne3lAx3S85wKwa230cpGzLGjZvkT7AztvzS2gxbXoG1T8LGF6C10b3vk651v4/1f4HK1a5uwckucZx6OYyY4t8XgYb9XjcoMPps93kwfcoShmnX3BriJy9t4DdvbmPiiEE88OmpFBdk9d4LHChzLZBDle4fUEuDd18PLY1R9jW4+5YGtx1sPvJ8aYPceMmo6e4b76hpMKibrSMDoRDsfBvWPAEfPgeNByAzH077lEsUo2ccnQwOlLmWx4a/wI6/u5bH4NFwyidc62PMTDeVfnc0HIDK911SqngPKlYf/YUjOR3GnAMlF8H4i2DYJPvy0AcsYZhjvLZ+D9948n1aWkP8+6cmMW9KxLkc4yPYCtVbYNdK2LXC3e/5AELeNCiDRsGoqUeSyMgpdgpwZ1Rh91rXklj3NBzcBSlZ7p/+5Oug5MLYTmCoq4ZNL7iWx9a/QrAJMvLg5Lmu5VFyUfQWQWOtSw4VXnKoXA01pUfKB4+BkWe41svIM93veuvrrnusar2rk1ngZl4uudC91pDREV4ojmrLYcc7LiEf2Hl02TH/V7Xz8kCS1/LO7HDftp3ZSVkGpGZBVkG3fgxLGCaiigMNfPXx93h3+36unz6aH115Ghmp/XTguqXBdVXsWnkkkezf7sokAIWneElkmkskuePcH40fA/HBFjeWU7cXDldBXZW3vddtH97rvomnD4K0wS6ZpQ9yraXw7XSvLG2Q25eS2btdPfu3uySx9imo2gCBZDjpYteSOHmOe3+6q+kwbHnVtTw2vQxNtS4JTbgYTrkCcoYf3XKo2Xrk2MGjYcQZLjGMnAIjznRr00dzsNIljtIlbjGww3vc/vyTjiSP4vPd+9lXQiHYtxF2vuMliXeg1puTLTXHzQ4tHVtDHX63x/yuwx6HWl0rvLnOffZbGqClzn2uYpFZAN/eevx6EVjCMFG1BkP8/NVNPLhkKxOGZvPATVOZMOwE+bZeVw0Vq6B8xZFE0tBherFk79tWapY7s6t9u+PjbEjNPLLd2nh0AqirOrLd8TXaXysdsoZCdqH759x4EJoOQtMhd388knQkoSRnuG/9gWR3O2Y7xSXD9u1k1zXUtr1rJZQvd887ZqZLEhOv6vwfc3e1NrsxrPV/gY2Lj/xDBxhU5JJCW2IYOaXb33wB901873qXOEqXwPa/u3+kEnBfFtq6r0ZO7d3xj9Zm10La+c6RW8N+V5Y1FMbOhDHnuvuhp3W/m64zqu7LSku9l0Dqw5JJ/ZGk0tLg3o8pN3XrZSxhmON6Y1MVX39iNYebWrl73ulcO62oaxMZ9geqrg981yo3htJcB82HvftIt7Cy1obIz5ma4xJAWyLICt8eCtlD3b7soS7RRHvPQiFoPuSSR3giaawN2w5LLi0N7ltmsAVCLd52q9sOtrjTXtu3vcdt28FWyB0Lp18Nk65xA9l9JRRyrb/Gg64Vke3zEgOtzS4xli5xXVgVq458C0/JdFP6Z+RBxhBvOxcy845st5eHPU5Jdy2o8uWwc6k7rbx8xZHPSN54L0F4t7ySAXU2oCUME5O9Bxv56uOreae0mksmDuOOOacwvjA73mH1jVDw6GSSlOKSQEoM06qY/qNhvztbb99GN6jesP/oW32Nuw+1RH+OlExobQINum/qwye51sOYc1yCGODXp1jCMDELhpQFb5Ry/18309ga4qYZY/jqxRMoyLZpQcwAoeq+FDTsd92LkZJKSqZLEEVnuS7CBGIJw3RZ1aEm7nttM48u30l6coDbZ43ntvNL+u+guDGmV3QlYdhJzgaAwpw0/vWq03n5ny/gvJMK+Okrm7jwv1/nT+/uJBgaOF8qjDHdZwnDHGV8YTYLPjudJ2+fycghGXzn6bXM/cWbvL5xLwOpNWqM6TpLGCais8bl8cyXzuXBT0+lsTXIrf/zLp/+7TLW7aqNd2jGmDjxNWGIyGwR2SgiW0TkjgjlXxeRD0VkjYi8JiJjw8qCIrLauy3yM04TmYgwd9IIXvnnWfzoiomsrzzI5b98i689/h7l++vjHZ4xpo/5NugtIknAJuASoBx4F7hRVT8Mq3MRsExV60XkS8CFqnq9V3ZYVbt0jqcNevvrYGMLDy3Zyu/e2oYqfO68cXz5wpMYnGnrZBhzouovg94zgC2qWqqqzcDjwLzwCqr6uqq2fVVdChT5GI/poUHpKXx79im8/s0LueKMkfzmzVIu+Mnr/PbNUppag/EOzxjjMz8TxiigLOxxubcvmi8AL4Q9TheRFSKyVESuinaQiMz36q2oqqrqWcQmJiOHZPDT687g+f93PmeMHsK/Pb+ec/79Ne56bh1ryg/Y4LgxA5QPE6C0i3TtfMT/JCJyMzAdmBW2e4yqVohICfBXEVmrqsfMrqWqC4AF4Lqkeh62idXEkYN45PMzeHvLPh5dvpPH3i3j4Xd2MGFoNldPK+KTZ45i2CBb38CYgcLPhFEOhM9HXARUdKwkIhcD3wNmqWpT235VrfDuS0VkCXAm0L3pGI2vzj2pgHNPKqC2oYXn11Ty1Moy7nlhA//14gY+OqGQa6YVcenEYT1fvMkYE1d+Dnon4wa9Pw7swg1636SqH4TVORN4CpitqpvD9ucC9araJCIFwDvAvPAB80hs0Lv/KK06zDOrdvHMqnIqahvJSU/m8skjuHpqEdPG5p54Ex0aM0D1m6lBRGQucC+QBCxU1R+LyN3AClVdJCKvApOASu+Qnap6pYicC/waCOHGWe5V1d8d7/UsYfQ/oZCytLSap1aW88K63TS0BBmXn8mnphbxqamjKMrNjHeIxiS0fpMw+poljP7tcFMrL6yt5OlV5SwtdWtMzCzJ5+ppRcw+fTjZaX72kBpjIrGEYfq9spp612X1Xjk7qutJSw4w6yOFzJ00go+dOpRB6XZthzF9wRKGOWGoKit37Ocvayp5cd1udh9sJDUpwPkTCpgzaQSXnDrMLgw0xkeWMMwJKRRS3is7wAtrK3lh3W52HWggOSCcd1IBn5g0gksmDiM3KzXeYRozoFjCMCc8VWVNeS2L11ayeF0lZTUNJAWEc8fnM+f0EVx62jBb5MmYXmAJwwwoqsoHFQdd8lhbyfbqegICZxfnM3fScC47fThDc+wCQWO6wxKGGbBUlQ27D/HC2kqeX1vJ1qo6RGDyqMGcXZLPjHF5nFWcx+AMG/cwJhaWMEzC2LznEIvX7ubvW/exeucBmoMhRODU4YOYUZzHOSV5nDUuj3zrvjImIksYJiE1tgRZXXaA5dtqWLatmpU79tPYEgJgwtBsZhTncXZJPmcX59kcV8Z4LGEYAzS3hli7q7Y9gazYvp/DTa0AjMvPdAmkOJ8ZxXmMzrMrzk1isoRhTAStwRDrKw+xbFs1y7bVsHxbDbUNLQDkZqYwriCLcfnerSDTu8+y8RAzoFnCMCYGoZCyae8hlpXWsHHPIbbvq2NHdT0VtQ2E/1nkZaUyNj+TYi+BjM3PpLjAbdsV6eZE15WEYZP3mIQVCAinDB/EKcMHHbW/sSXIzpp6tu+rY3t1Hdv2ue2lpdU8896uo+rmZaUyLj+TotxMRgxOZ9igdIYP9m6D0hmak0Zykp/rlBnTdyxhGNNBekoSHxmWw0eG5RxT1tgSZEd1Pdur68ISSh2ryw7w4geNNLeGjqovAoXZaQz3kkl7Umnb9hJLlk28aE4A9ik1pgvSU5I4eXgOJw8/NpmoKvvrW9hd28ieg41U1jay+2Aju2sb2H2wiZ3V9SwrreZgY+sxx6YlB8hJTyY7LZmsNHefnZZMdrp7nJMWVpZ+dHl2WjLpyUkkJwnJASEpICQHAiR5j9v22RokpqcsYRjTS0SEvKxU8rJSmThyUNR6Dc1Bdh9spLK2oT2xHKhv4XBTK4cbW6lrauVQUyu7DzZyuMrtO9zUSlOH1ktXJbUnE++WFGh/nBR2Sw4IARGSk4Qk6VgWIBBex9s/ODOFkoIsiguyKCnMpig3gxTrihtwLGEY08cyUpMo9v65dkVLMOSSSWMrdc0ukRzykkxTa4hgKERrSAmGlNag0tr2OKjt+1tCoaMeu/sQrUElqO5xKNThXt3zBUNKQzB4TJ2WUIiaumYO1Le0x5ocEMbkZXoJJIviguz27aE5adbaOUH5mjBEZDbwC9yKe79V1Xs6lKcBjwDTgGrgelXd7pXdCXwBCAL/pKov+RmrMf1dSlKAIZmpDMnsnzP27q9rpnSfG9MprTrMNm/7rS37jmodZaUmURyWRMYXZlGY7ZJIQNzJCAJHHotrzYi4MaG2xwFxdURcgkpNDpCSFCA1OUBqkrsFApaYepNvCUNEkoAHgEuAcuBdEVnUYV3uLwD7VfUkEbkB+E/gehGZCNwAnAaMBF4VkY+oatCveI0xPZOblcq0rFSmjc09an8opFTUNrQnkNKqOkr31bG6bD9/WVOBn2f2tyWStiSSkhQgzXscnlzSUgJkpCSRkZJEeqq7z0xNIt3bl+HtSw/bdvsDpKckkZocICCuCy8gggRoT2rhCe5I4jsxE5mfLYwZwBZVLQUQkceBeUB4wpgH/Mjbfgq4X9w7OQ94XFWbgG0issV7vnd8jNcY44NAQCjKdacenz+h8KiytlOYa+qaUXUnDoQUQqoo3r0qoZDbDinAkToh75jWoNIcDNESDNHcGqKp1d23PW7ueO9tt5UfPtxKY0uQhpagu28OUt8S9C2ZiXAkuQjHJJRA4OhWVMeWVsf6+VlpPHH7TH+CDeNnwhgFlIU9LgfOjlZHVVtFpBbI9/Yv7XDsqEgvIiLzgfkAY8aM6ZXAjTF9o+0U5v5I1SWhxuYQDV4yaWg+Oqm07W9qDXmJLSzhKQRV27fDy47cvMchr46GJckOSTG8voaVhVTJ6aPTsv18lUhtro75OlqdWI51O1UXAAvAXendlQCNMSYaESEtOYm05CQGY1f0A/h53ls5MDrscRFQEa2OiCQDg4GaGI81xhjTh/xMGO8CE0SkWERScYPYizrUWQTc4m1fA/xV3eRWi4AbRCRNRIqBCcByH2M1xhhzHL51SXljEl8BXsKdVrtQVT8QkbuBFaq6CPgd8L/eoHYNLqng1XsCN0DeCnzZzpAyxpj4stlqjTEmgXVltlq7dt8YY0xMLGEYY4yJiSUMY4wxMbGEYYwxJiYDatBbRKqAHd08vADY14vh9DaLr2csvp6x+HqmP8c3VlULj19tgCWMnhCRFbGeKRAPFl/PWHw9Y/H1TH+PL1bWJWWMMSYmljCMMcbExBLGEQviHcBxWHw9Y/H1jMXXM/09vpjYGIYxxpiYWAvDGGNMTCxhGGOMiUnCJQwRmS0iG0Vki4jcEaE8TUT+5JUvE5FxfRjbaBF5XUTWi8gHIvLVCHUuFJFaEVnt3X7YV/F5r79dRNZ6r33MTI/i3Oe9f2tEZGofxnZy2PuyWkQOisjXOtTp0/dPRBaKyF4RWRe2L09EXhGRzd59bpRjb/HqbBaRWyLV8Sm+n4jIBu/396yIDIlybKefBR/j+5GI7Ar7Hc6Ncmynf+s+xvensNi2i8jqKMf6/v71OvWWA0yEG26a9a1ACZAKvA9M7FDnH4GHvO0bgD/1YXwjgKnedg6wKUJ8FwJ/ieN7uB0o6KR8LvACbtXEc4Blcfxd78ZdlBS39w+4AJgKrAvb91/AHd72HcB/RjguDyj17nO97dw+iu9SINnb/s9I8cXyWfAxvh8B34zh99/p37pf8XUo/ynww3i9f719S7QWxgxgi6qWqmoz8Dgwr0OdecDD3vZTwMdFJNKSsb1OVStVdZW3fQhYT5S1zPuxecAj6iwFhojIiDjE8XFgq6p298r/XqGqb+DWegkX/hl7GLgqwqGXAa+oao2q7gdeAWb3RXyq+rKqtnoPl+JWvIyLKO9fLGL5W++xzuLz/m9cBzzW268bL4mWMEYBZWGPyzn2H3J7He+PphbI75PownhdYWcCyyIUzxSR90XkBRE5rU8Dc2urvywiK0VkfoTyWN7jvnAD0f9Q4/n+AQxT1UpwXxKAoRHq9Jf38fO4FmMkx/ss+OkrXpfZwihdev3h/Tsf2KOqm6OUx/P965ZESxiRWgodzyuOpY6vRCQbeBr4mqoe7FC8CtfNcgbwS+DPfRkbcJ6qTgXmAF8WkQs6lPeH9y8VuBJ4MkJxvN+/WPWH9/F7uBUv/xilyvE+C375FTAemAJU4rp9Oor7+wfcSOeti3i9f92WaAmjHBgd9rgIqIhWR0SSgcF0r0ncLSKSgksWf1TVZzqWq+pBVT3sbS8GUkSkoK/iU9UK734v8Cyu6R8ulvfYb3OAVaq6p2NBvN8/z562bjrvfm+EOnF9H71B9suBT6vX4d5RDJ8FX6jqHlUNqmoI+E2U1433+5cMfAr4U7Q68Xr/eiLREsa7wAQRKfa+hd4ALOpQZxHQdkbKNcBfo/3B9Davz/N3wHpV/VmUOsPbxlREZAbud1jdR/FliUhO2zZucHRdh2qLgM96Z0udA9S2db/0oajf7OL5/oUJ/4zdAjwXoc5LwKUikut1uVzq7fOdiMwGvgNcqar1UerE8lnwK77wMbFPRnndWP7W/XQxsEFVyyMVxvP965F4j7r39Q13Fs8m3BkU3/P23Y374wBIx3VlbAGWAyV9GNtHcc3mNcBq7zYXuB243avzFeAD3FkfS4Fz+zC+Eu913/diaHv/wuMT4AHv/V0LTO/j328mLgEMDtsXt/cPl7gqgRbct94v4MbEXgM2e/d5Xt3pwG/Djv289zncAtzah/FtwfX/t30G284aHAks7uyz0Efx/a/32VqDSwIjOsbnPT7mb70v4vP2/77tMxdWt8/fv96+2dQgxhhjYpJoXVLGGGO6yRKGMcaYmFjCMMYYExNLGMYYY2JiCcMYY0xMLGEY0w94s+j+Jd5xGNMZSxjGGGNiYgnDmC4QkZtFZLm3hsGvRSRJRA6LyE9FZJWIvCYihV7dKSKyNGxdiVxv/0ki8qo3AeIqERnvPX22iDzlrUXxx76aJdmYWFnCMCZGInIqcD1u0rgpQBD4NJCFm7tqKvA34C7vkEeA76jqZNyVyW37/wg8oG4CxHNxVwqDm534a8BE3JXA5/n+QxnTBcnxDsCYE8jHgWnAu96X/wzcxIEhjkwy9wfgGREZDAxR1b95+x8GnvTmDxqlqs8CqGojgPd8y9Wbe8hbpW0c8Jb/P5YxsbGEYUzsBHhYVe88aqfIDzrU62y+nc66mZrCtoPY36fpZ6xLypjYvQZcIyJDoX1t7rG4v6NrvDo3AW+pai2wX0TO9/Z/BvibuvVNykXkKu850kQks09/CmO6yb7BGBMjVf1QRL6PWyUtgJuh9MtAHXCaiKzErdB4vXfILcBDXkIoBW719n8G+LWI3O09x7V9+GMY0202W60xPSQih1U1O95xGOM365IyxhgTE2thGGOMiYm1MIwxxsTEEoYxxpiYWMIwxhgTE0sYxhhjYmIJwxhjTEz+Pwv0x5qw0qsiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "if history is not None:\n",
    "    print(history.history.keys())\n",
    "    %matplotlib inline\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    %matplotlib inline\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.4 循环神经网络(RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.4.1 RNN概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1) BP算法,CNN之后, 为什么还有RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "细想BP算法,CNN(卷积神经网络)我们会发现, 他们的输出都是只考虑前一个输入的影响而不考虑其它时刻输入的影响, 比如简单的猫,狗,手写数字等单个物体的识别具有较好的效果。\n",
    "\n",
    "但是, 对于一些与时间先后有关的, 比如视频的下一时刻的预测,文档前后文内容的预测等, 这些算法的表现就不尽如人意了.因此, RNN就应运而生了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 什么是RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN是一种特殊的神经网络结构, 它是根据\"人的认知是基于过往的经验和记忆\"这一观点提出的。\n",
    "\n",
    "它与DNN,CNN不同的是: 它不仅考虑前一时刻的输入,而且赋予了网络对前面的内容的一种'记忆'功能。\n",
    "\n",
    "RNN之所以称为循环神经网络，即一个序列当前的输出与前面的输出也有关。\n",
    "\n",
    "具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) RNN的主要应用领域有哪些呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN的应用领域有很多, 可以说只要考虑时间先后顺序的问题都可以使用RNN来解决.这里主要说一下几个常见的应用领域:\n",
    "\n",
    "   ① <font color='red'>自然语言处理(NLP)</font>: 主要有视频处理, 文本生成, 语言模型, 图像处理\n",
    "\n",
    "   ② 机器翻译, 机器写小说\n",
    "\n",
    "   ③ 语音识别\n",
    "\n",
    "   ④ 图像描述生成\n",
    "\n",
    "   ⑤ 文本相似度计算\n",
    "\n",
    "   ⑥ 音乐推荐、网易考拉商品推荐、Youtube视频推荐等新的应用领域."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.4.2 RNN详细介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.4.2.1 RNN模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们说了RNN具有时间\"记忆\"的功能, 那么它是怎么实现所谓的\"记忆\"的呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_1.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图1所示, 我们可以看到RNN层级结构较之于CNN来说比较简单, 它主要有输入层,Hidden Layer, 输出层组成。\n",
    "\n",
    "注意：图中没有卷积层与池化层。\n",
    "\n",
    "并且会发现在Hidden Layer 有一个箭头表示数据的循环更新, 这个就是实现时间记忆功能的方法.\n",
    "\n",
    "如果到这里你还是没有搞懂RNN到底是什么意思,那么请继续往下看!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图所示为Hidden Layer的层级展开图。\n",
    "\n",
    "t-1, t, t+1表示时间序列 \n",
    "\n",
    "X表示输入的样本。 \n",
    "\n",
    "S<sub>t</sub>表示样本在时间t处的的记忆。\n",
    "\n",
    "S<sub>t</sub> = f(W\\*S<sub>t-1</sub> +U\\*X<sub>t</sub>)。\n",
    "\n",
    "W表示输入的权重\n",
    "\n",
    "U表示此刻输入的样本的权重\n",
    "\n",
    "V表示输出的样本权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在t =1时刻, 一般初始化输入S<sub>0</sub>=0, 随机初始化W,U,V, 进行下面的公式计算:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中,f和g均为激活函数. 其中f可以是tanh,relu,sigmoid等激活函数，g通常是softmax也可以是其他。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 时间就向前推进，此时的状态s<sub>1</sub>作为时刻1的记忆状态将参与下一个时刻的预测活动，也就是:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以此类推, 可以得到最终的输出值为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f3.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意: \n",
    "\n",
    "1. 这里的W,U,V在每个时刻都是相等的(权重共享).\n",
    "\n",
    "2. 隐藏状态可以理解为:  S=f(现有的输入+过去记忆总结) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.4.2.2 RNN反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们介绍了RNN的前向传播的方式, 那么RNN的权重参数W,U,V都是怎么更新的呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一次的输出值O<sub>t</sub>都会产生一个误差值E<sub>t</sub>, 则总的误差可以表示为:<img src='./image/rnn_f4.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则<b>损失函数</b>可以使用<b>交叉熵损失函数</b>也可以使用<b>平方误差损失函数</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于每一步的输出不仅仅依赖当前步的网络，并且还需要前若干步网络的状态，那么这种BP改版的算法叫做<b>Backpropagation Through Time(BPTT)</b> , 也就是将输出端的误差值反向传递,运用梯度下降法进行更新."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">为什么提到梯度就要求偏导数？\n",
    "毕竟求偏导的物理意义是：表示在某一方向速度随位置变化的快慢（类似加速度）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是要求参数的梯度:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f5.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们求解W的更新方法, 由前面的W的更新可以看出它是每个时刻的偏差的偏导数之和. \n",
    "\n",
    "在这里我们以 t = 3时刻为例, 根据链式求导法则可以得到t = 3时刻的偏导数为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f6.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时, 根据公式<img src='./image/rnn_f7.png' />， 我们会发现, S3除了和W有关之外, 还和前一时刻S2有关.\n",
    "\n",
    "对于S3直接展开得到下面的式子:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f8.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于S2直接展开得到下面的式子:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f9.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于S1直接展开得到下面的式子:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f10.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 将上述三个式子合并得到:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f11.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就得到了公式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f12.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里要说明的是:<img src='./image/rnn_f13.png' />，<br>表示的是S3对W直接求导, 不考虑S2的影响.(也就是例如y = f(x)*g(x)对x求导一样)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其次是对U的更新方法. 由于参数U求解和W求解类似,这里就不在赘述了,最终得到的具体的公式如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f14.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后,给出V的更新公式(V只和输出O有关):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/rnn_f15.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN的特性：\n",
    "\n",
    "a）循环神经网络能够在每个时间节点产生一个输出，且隐单元间的连接是循环的；\n",
    "\n",
    "b）循环神经网络能够在每个时间节点产生一个输出，且该时间节点上的输出仅与下一时间节点的隐单元有循环连接；\n",
    "\n",
    "c）循环神经网络包含带有循环连接的隐单元，且能够处理序列数据并输出单一的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.4.3 RNN的一些改进算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们介绍了RNN的算法, 它处理时间序列的问题的效果很好, 但是仍然存在着一些问题, 其中较为严重的是容易出现梯度消失或者梯度爆炸的问题(BP算法和长时间依赖造成的). 注意: 这里的梯度消失和BP的不一样,这里主要指由于时间过长而造成记忆值较小的现象.\n",
    "\n",
    "因此, 就出现了一系列的改进的算法, 这里介绍主要的两种算法: LSTM 和 GRU.\n",
    "\n",
    "LSTM 和 GRU对于梯度消失或者梯度爆炸的问题处理方法主要是:\n",
    "\n",
    "对于<b>梯度消失</b>: 由于它们都有特殊的方式存储”记忆”，那么以前梯度比较大的”记忆”不会像简单的RNN一样马上被抹除，因此可以一定程度上克服梯度消失问题。\n",
    "\n",
    "对于<b>梯度爆炸</b>:用来克服梯度爆炸的问题就是gradient clipping，也就是当你计算的梯度超过阈值c或者小于阈值-c的时候，便把此时的梯度设置成c或-c。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.4.3.1 LSTM算法（Long Short Term Memory, 长短期记忆网络）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM是目前使用最多的时间序列算法，几乎算是标配了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lstm_1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和RNN不同的是，RNN中：<img src='./image/rnn_f.png' />，就是个简单的线性求和的过程，而LSTM可以通过“门”结构来去除或增加“细胞状态”的信息。\n",
    "\n",
    "从而实现了对重要内容的保留，和对不重要内容的去除，通过Sigmoid层输出一个0到1之间的概率值，描述每个部分有多少量可以通过。\n",
    "\n",
    "0：表示“不允许任务变量通过”\n",
    "1：表示“运行所有变量通过”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM的巧妙之处在于通过增加输入门限，遗忘门限和输出门限，使得自循环的权重是变化的，这样一来在模型参数固定的情况下，不同时刻的积分尺度可以动态改变，从而避免了梯度消失或者梯度膨胀的问题。\n",
    "\n",
    ">用于遗忘的门叫做\"遗忘门\", 用于信息增加的叫做\"信息增加门\",最后是用于输出的\"输出门\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lstm_4.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外,LSTM算法的还有一些变种。\n",
    "\n",
    "如图，它增加“peephole connections”层，让门层也接受细胞状态的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lstm_2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图所示为LSTM的另外一种变种算法.它是通过耦合忘记门和更新输入门(第一个和第二个门)；也就是不再单独的考虑忘记什么、增加什么信息，而是一起进行考虑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/lstm_3.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.4.3.2 GRU算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN与RNN能否联合起来用呢？有的，这个称之为RCNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/cnnrnn.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/netrocompare.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 样本准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
